# aws_s3_bucket.clickhouse_data:
resource "aws_s3_bucket" "clickhouse_data" {
    arn                         = "arn:aws:s3:::clickhouse-s3-direct-test-20251205"
    bucket                      = "clickhouse-s3-direct-test-20251205"
    bucket_domain_name          = "clickhouse-s3-direct-test-20251205.s3.amazonaws.com"
    bucket_regional_domain_name = "clickhouse-s3-direct-test-20251205.s3.ap-northeast-2.amazonaws.com"
    force_destroy               = false
    hosted_zone_id              = "Z3W03O7B5YMIYP"
    id                          = "clickhouse-s3-direct-test-20251205"
    object_lock_enabled         = false
    region                      = "ap-northeast-2"
    request_payer               = "BucketOwner"
    tags                        = {
        "Environment" = "dev"
        "Name"        = "clickhouse-s3-direct-test-20251205"
        "Purpose"     = "ClickHouse S3 Table Engine"
    }
    tags_all                    = {
        "Environment" = "dev"
        "Name"        = "clickhouse-s3-direct-test-20251205"
        "Purpose"     = "ClickHouse S3 Table Engine"
    }

    grant {
        id          = "5cafa167c3958e9e2dd3565aac0a7941ce5499a0f241ad2a4e5e5c8b6c595f65"
        permissions = [
            "FULL_CONTROL",
        ]
        type        = "CanonicalUser"
    }

    server_side_encryption_configuration {
        rule {
            bucket_key_enabled = false

            apply_server_side_encryption_by_default {
                sse_algorithm = "AES256"
            }
        }
    }

    versioning {
        enabled    = false
        mfa_delete = false
    }
}

# aws_s3_bucket_policy.clickhouse_access:
resource "aws_s3_bucket_policy" "clickhouse_access" {
    bucket = "clickhouse-s3-direct-test-20251205"
    id     = "clickhouse-s3-direct-test-20251205"
    policy = jsonencode(
        {
            Statement = [
                {
                    Action    = [
                        "s3:GetBucketLocation",
                        "s3:ListBucket",
                    ]
                    Effect    = "Allow"
                    Principal = {
                        AWS = [
                            "arn:aws:iam::277707138598:role/CH-S3-orangeaws-ba-92-an2-fb-Role",
                        ]
                    }
                    Resource  = "arn:aws:s3:::clickhouse-s3-direct-test-20251205"
                    Sid       = "ClickHouseDirectBucketAccess"
                },
                {
                    Action    = [
                        "s3:GetObject",
                        "s3:GetObjectVersion",
                        "s3:ListMultipartUploadParts",
                        "s3:PutObject",
                        "s3:DeleteObject",
                        "s3:AbortMultipartUpload",
                    ]
                    Effect    = "Allow"
                    Principal = {
                        AWS = [
                            "arn:aws:iam::277707138598:role/CH-S3-orangeaws-ba-92-an2-fb-Role",
                        ]
                    }
                    Resource  = "arn:aws:s3:::clickhouse-s3-direct-test-20251205/*"
                    Sid       = "ClickHouseDirectObjectAccess"
                },
            ]
            Version   = "2012-10-17"
        }
    )
}

# aws_s3_bucket_public_access_block.clickhouse_data_public_access:
resource "aws_s3_bucket_public_access_block" "clickhouse_data_public_access" {
    block_public_acls       = true
    block_public_policy     = false
    bucket                  = "clickhouse-s3-direct-test-20251205"
    id                      = "clickhouse-s3-direct-test-20251205"
    ignore_public_acls      = true
    restrict_public_buckets = false
}

# aws_s3_bucket_server_side_encryption_configuration.clickhouse_data_encryption:
resource "aws_s3_bucket_server_side_encryption_configuration" "clickhouse_data_encryption" {
    bucket = "clickhouse-s3-direct-test-20251205"
    id     = "clickhouse-s3-direct-test-20251205"

    rule {
        apply_server_side_encryption_by_default {
            sse_algorithm = "AES256"
        }
    }
}

# aws_s3_bucket_versioning.clickhouse_data_versioning:
resource "aws_s3_bucket_versioning" "clickhouse_data_versioning" {
    bucket = "clickhouse-s3-direct-test-20251205"
    id     = "clickhouse-s3-direct-test-20251205"

    versioning_configuration {
        status = "Enabled"
    }
}

# aws_s3_object.sample_folders["data/"]:
resource "aws_s3_object" "sample_folders" {
    arn                    = "arn:aws:s3:::clickhouse-s3-direct-test-20251205/data/"
    bucket                 = "clickhouse-s3-direct-test-20251205"
    bucket_key_enabled     = false
    content_type           = "application/x-directory"
    etag                   = "d41d8cd98f00b204e9800998ecf8427e"
    force_destroy          = false
    id                     = "data/"
    key                    = "data/"
    server_side_encryption = "AES256"
    storage_class          = "STANDARD"
    tags_all               = {}
}

# aws_s3_object.sample_folders["exports/"]:
resource "aws_s3_object" "sample_folders" {
    arn                    = "arn:aws:s3:::clickhouse-s3-direct-test-20251205/exports/"
    bucket                 = "clickhouse-s3-direct-test-20251205"
    bucket_key_enabled     = false
    content_type           = "application/x-directory"
    etag                   = "d41d8cd98f00b204e9800998ecf8427e"
    force_destroy          = false
    id                     = "exports/"
    key                    = "exports/"
    server_side_encryption = "AES256"
    storage_class          = "STANDARD"
    tags_all               = {}
}

# aws_s3_object.sample_folders["logs/"]:
resource "aws_s3_object" "sample_folders" {
    arn                    = "arn:aws:s3:::clickhouse-s3-direct-test-20251205/logs/"
    bucket                 = "clickhouse-s3-direct-test-20251205"
    bucket_key_enabled     = false
    content_type           = "application/x-directory"
    etag                   = "d41d8cd98f00b204e9800998ecf8427e"
    force_destroy          = false
    id                     = "logs/"
    key                    = "logs/"
    server_side_encryption = "AES256"
    storage_class          = "STANDARD"
    tags_all               = {}
}


Outputs:

bucket_arn = "arn:aws:s3:::clickhouse-s3-direct-test-20251205"
bucket_domain_name = "clickhouse-s3-direct-test-20251205.s3.amazonaws.com"
bucket_name = "clickhouse-s3-direct-test-20251205"
bucket_region = "ap-northeast-2"
clickhouse_iam_role_arns = [
    "arn:aws:iam::277707138598:role/CH-S3-orangeaws-ba-92-an2-fb-Role",
]
clickhouse_sql_examples = <<-EOT
    -- Example 1: Create S3-backed table (Parquet format) - Direct Access
    CREATE TABLE logs_s3
    (
        timestamp DateTime,
        level String,
        message String
    )
    ENGINE = S3(
        'https://s3.ap-northeast-2.amazonaws.com/clickhouse-s3-direct-test-20251205/logs/app_logs.parquet',
        'Parquet'
    );
    
    -- Example 2: Create S3-backed table (CSV format) - Direct Access
    CREATE TABLE events_s3
    (
        event_id UInt64,
        user_id String,
        event_type String,
        created_at DateTime
    )
    ENGINE = S3(
        'https://s3.ap-northeast-2.amazonaws.com/clickhouse-s3-direct-test-20251205/data/events.csv',
        'CSV'
    );
    
    -- Example 3: Create S3-backed table (JSON format with wildcard) - Direct Access
    CREATE TABLE user_activity_s3
    (
        user_id String,
        action String,
        timestamp DateTime
    )
    ENGINE = S3(
        'https://s3.ap-northeast-2.amazonaws.com/clickhouse-s3-direct-test-20251205/data/user_activity_*.json',
        'JSONEachRow'
    );
    
    -- Example 4: Direct query without creating table - Direct Access
    SELECT * FROM s3(
        'https://s3.ap-northeast-2.amazonaws.com/clickhouse-s3-direct-test-20251205/data/*.parquet'
    ) LIMIT 10;
    
    -- Example 5: Insert data to S3
    INSERT INTO logs_s3 VALUES
        (now(), 'INFO', 'Application started'),
        (now(), 'DEBUG', 'Processing request'),
        (now(), 'ERROR', 'Connection timeout');
    
    -- Example 6: Export query results to S3 - Direct Access
    INSERT INTO FUNCTION s3(
        'https://s3.ap-northeast-2.amazonaws.com/clickhouse-s3-direct-test-20251205/exports/query_result.parquet',
        'Parquet',
        'user_id String, total_events UInt64'
    )
    SELECT user_id, count() AS total_events
    FROM user_activity_s3
    GROUP BY user_id;
EOT
connection_info = <<-EOT
    ================================================================================
    ClickHouse Cloud S3 Configuration (Direct Bucket Policy Access)
    ================================================================================
    
    S3 Bucket Information:
      Bucket Name:       clickhouse-s3-direct-test-20251205
      Bucket ARN:        arn:aws:s3:::clickhouse-s3-direct-test-20251205
      Region:            ap-northeast-2
      S3 URL:            https://s3.ap-northeast-2.amazonaws.com/clickhouse-s3-direct-test-20251205
    
    Access Method:
      Type:              Direct S3 Bucket Policy (No AssumeRole)
      Allowed ARNs:      arn:aws:iam::277707138598:role/CH-S3-orangeaws-ba-92-an2-fb-Role
    
    ClickHouse S3 Table Engine Example (No extra_credentials needed):
      -- Create table with S3 engine (direct access)
      CREATE TABLE s3_table
      (
          id UInt64,
          name String,
          timestamp DateTime
      )
      ENGINE = S3(
          'https://s3.ap-northeast-2.amazonaws.com/clickhouse-s3-direct-test-20251205/data/table_data.parquet',
          'Parquet'
      );
    
      -- Insert data to S3
      INSERT INTO s3_table VALUES (1, 'example', now());
    
      -- Query data from S3
      SELECT * FROM s3_table;
    
    ClickHouse S3 Function Example (No extra_credentials needed):
      -- Query Parquet file directly
      SELECT * FROM s3(
          'https://s3.ap-northeast-2.amazonaws.com/clickhouse-s3-direct-test-20251205/data/*.parquet'
      );
    
      -- Insert data to S3 using s3() function
      INSERT INTO FUNCTION s3(
          'https://s3.ap-northeast-2.amazonaws.com/clickhouse-s3-direct-test-20251205/data/output.parquet',
          'Parquet',
          'id UInt64, name String, timestamp DateTime'
      )
      SELECT 1 AS id, 'test' AS name, now() AS timestamp;
    
    ================================================================================
EOT
s3_url_prefix = "https://s3.ap-northeast-2.amazonaws.com/clickhouse-s3-direct-test-20251205"
setup_checklist = <<-EOT
    ================================================================================
    Setup Checklist (Direct S3 Bucket Policy Access)
    ================================================================================
    
    AWS Setup (Completed by Terraform):
      ✓ S3 bucket created: clickhouse-s3-direct-test-20251205
      ✓ S3 bucket policy created with direct ARN access
      ✓ S3 permissions configured (read + write)
      ✓ Bucket encryption enabled
      ✓ Public access settings configured
    
    ClickHouse Cloud Setup (Manual Steps):
      1. Log into ClickHouse Cloud Console
      2. Select your service
      3. Navigate to: Settings → Network security information
      4. Copy the "Service role ID (IAM)" value
      5. Verify it matches one of the ARNs in your terraform.tfvars:
         arn:aws:iam::277707138598:role/CH-S3-orangeaws-ba-92-an2-fb-Role
    
    Testing Steps:
      1. Connect to your ClickHouse Cloud instance
      2. Copy SQL examples from the 'clickhouse_sql_examples' output
      3. Execute the SQL to create S3-backed tables (NO extra_credentials needed!)
      4. Insert test data
      5. Query the data to verify
      6. Check S3 bucket for created files
    
    Key Differences from AssumeRole Method:
      • No IAM role creation - uses direct S3 bucket policy
      • No extra_credentials() needed in SQL queries
      • Simpler SQL syntax
      • Direct access from ClickHouse IAM role to S3 bucket
    
    Troubleshooting:
      - If you get "Access Denied" errors:
        • Verify ClickHouse IAM role ARN is correct
        • Check S3 bucket policy in AWS Console
        • Ensure S3 bucket is in the same region as ClickHouse Cloud
      - If you get "Bucket not found" errors:
        • Verify S3 URL format
        • Check bucket name spelling
        • Ensure bucket region matches
    
    ================================================================================
EOT
