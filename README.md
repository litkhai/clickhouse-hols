# ClickHouse Hands-On Labs (HOLs)

A collection of practical, hands-on laboratory exercises for learning and exploring ClickHouse - the fast open-source column-oriented database management system.

## üéØ Purpose

These hands-on labs are designed to provide practical experience with:
- **ClickHouse OSS** (Open Source Software)
- **ClickHouse Cloud** (Managed service)

Whether you're a beginner learning ClickHouse fundamentals or an experienced user exploring advanced features, these labs offer structured, step-by-step exercises to build your skills with real-world scenarios.

## üìö Available Labs

### 1. [oss-mac-setup](oss-mac-setup/)
**Created:** November 2025
**Purpose:** Quick setup for running ClickHouse OSS (Open Source) on macOS

Development environment optimized for macOS with Docker, featuring:
- Custom seccomp security profile to fix `get_mempolicy` errors on macOS
- Version control with support for specific ClickHouse versions or latest
- Docker named volumes for persistent data storage
- Easy management scripts for start/stop/cleanup operations
- Multiple access interfaces (Web UI, HTTP API, TCP)

**Use Cases:**
- Local ClickHouse development on macOS
- Testing ClickHouse features before cloud deployment
- Learning ClickHouse fundamentals in a safe local environment

**Quick Start:**
```bash
cd oss-mac-setup
./set.sh        # Setup with latest version
./start.sh      # Start ClickHouse
./client.sh     # Connect to CLI
```

---

### 2. [tpcds](tpcds/)
**Created:** November 2025 *(Under Development)*
**Purpose:** TPC-DS benchmark for ClickHouse performance testing

Industry-standard decision support benchmark for evaluating ClickHouse performance:
- Complete TPC-DS schema with 24 tables (7 fact tables, 17 dimension tables)
- 99 analytical query templates covering various patterns
- Automated data generation and loading scripts
- Sequential and parallel query execution
- Performance metrics and result analysis

**Use Cases:**
- Benchmarking ClickHouse performance at different scale factors
- Testing query optimization and tuning
- Comparing performance across different ClickHouse versions or configurations
- Stress testing production environments

**Quick Start:**
```bash
cd tpcds
./00-set.sh --interactive     # Configure environment
./01-create-schema.sh         # Create schema
./03-load-data.sh --source s3 # Load data from S3
./04-run-queries-sequential.sh # Run benchmark
```

---

### 3. [datalake-minio-catalog](datalake-minio-catalog/)
**Created:** November 2025
**Purpose:** Local data lake environment with MinIO and multiple catalog options

Complete data lake stack running locally with Docker:
- **MinIO**: S3-compatible object storage for data lake storage
- **Multiple Catalog Options**: Nessie (Git-like), Hive Metastore, or Iceberg REST
- **Apache Iceberg**: Modern table format with ACID guarantees
- **Jupyter Notebooks**: Interactive data exploration with pre-configured examples
- **Sample Data**: Pre-loaded JSON and Parquet datasets

**Use Cases:**
- Learning data lake architecture and Apache Iceberg concepts
- Testing ClickHouse integration with S3-compatible storage
- Experimenting with different catalog implementations
- Prototyping data lake solutions before cloud deployment

**Quick Start:**
```bash
cd datalake-minio-catalog
./setup.sh --configure  # Choose catalog type
./setup.sh --start      # Start all services
# Access Jupyter at http://localhost:8888
# Access MinIO Console at http://localhost:9001
```

---

### 4. [terraform-minio-on-aws](terraform-minio-on-aws/)
**Created:** November 2025
**Purpose:** Deploy single-node MinIO server on AWS EC2 with Terraform

Production-ready MinIO deployment on AWS infrastructure:
- **Ubuntu 22.04 LTS**: Following MinIO's official recommendations
- **Automated Deployment**: One-command deployment with `deploy.sh`
- **Configurable Resources**: Instance type and EBS volume size
- **Security Groups**: Pre-configured firewall rules for MinIO API/Console/SSH
- **Optional Elastic IP**: Stable public IP for production use
- **Health Monitoring**: Automated health checks and detailed installation logs

**Use Cases:**
- Deploying S3-compatible object storage on AWS
- Creating MinIO infrastructure for ClickHouse Cloud integration
- Testing ClickHouse S3 functions with real cloud storage
- Cost-effective alternative to AWS S3 for specific workloads

**Quick Start:**
```bash
cd terraform-minio-on-aws
export AWS_ACCESS_KEY_ID="your-key"
export AWS_SECRET_ACCESS_KEY="your-secret"
./deploy.sh   # Automated deployment
./destroy.sh  # Cleanup when done
```

---

### 5. [terraform-glue-s3-chc-integration](terraform-glue-s3-chc-integration/)
**Created:** November 2025
**Purpose:** ClickHouse Cloud integration with AWS Glue Catalog using Apache Iceberg

**‚ö†Ô∏è Important:** This lab demonstrates ClickHouse Cloud 25.8's current DataLakeCatalog capabilities with known limitations.

Automated infrastructure for ClickHouse Cloud + AWS Glue + Iceberg integration:
- **S3 Storage**: Encrypted bucket for Apache Iceberg data
- **AWS Glue Database**: Metadata catalog for Iceberg tables
- **PyIceberg**: Proper Iceberg v2 table creation with Glue catalog support
- **Sample Data**: Pre-configured `sales_orders` table with partitioning
- **One-Command Deployment**: Automated setup with `deploy.sh`
- **Credential Management**: Secure environment variable handling

**Current Limitations (ClickHouse Cloud 25.8):**
- ‚ùå `glue_database` parameter not supported in DataLakeCatalog
- ‚ùå IAM role-based authentication not supported (must use access keys)
- ‚úÖ DataLakeCatalog automatically discovers all Glue databases in the region

**Future Enhancements:** Once ClickHouse Cloud adds support for `glue_database` and IAM roles, this setup will be updated accordingly.

**Use Cases:**
- Integrating ClickHouse Cloud with AWS Glue Data Catalog
- Querying Apache Iceberg tables from ClickHouse
- Database-level catalog integration (mount entire Glue database)
- Testing DataLakeCatalog engine capabilities and limitations

**Quick Start:**
```bash
cd terraform-glue-s3-chc-integration
./deploy.sh  # Prompts for AWS credentials, deploys everything
# Copy the SQL output and run in ClickHouse Cloud console
./destroy.sh # Cleanup when done
```

**Technical Architecture:**
```
ClickHouse Cloud (DataLakeCatalog)
    ‚Üì (AWS Credentials)
AWS Glue Catalog (clickhouse_iceberg_db)
    ‚Üì (Table Metadata)
S3 Bucket (Apache Iceberg Data)
    ‚Üì (Parquet Files)
Sample Table: sales_orders (10 records, partitioned by date)
```

---

### 6. [terraform-chc-secures3-aws](terraform-chc-secures3-aws/)
**Created:** November 2025
**Purpose:** Secure ClickHouse Cloud S3 integration using IAM role-based authentication

Production-ready S3 access for ClickHouse Cloud without managing access keys:
- **IAM Role-Based Authentication**: Secure access using AWS IAM role assumption
- **Read & Write Permissions**: Full support for SELECT, INSERT, and export operations
- **S3 Table Engine Support**: Create tables backed by S3 storage
- **Multiple Format Support**: Parquet, CSV, JSON, and more
- **Automated Deployment**: One-command deployment with `deploy.sh`
- **Production Security**: Encryption, versioning, and public access blocking

**Use Cases:**
- Secure S3 access for ClickHouse Cloud without access keys
- Creating S3-backed tables for cost-effective cold storage
- Exporting ClickHouse query results to S3
- Querying large datasets directly from S3
- Building data lake architectures with ClickHouse Cloud

**Quick Start:**
```bash
cd terraform-chc-secures3-aws
export AWS_ACCESS_KEY_ID="your-key"
export AWS_SECRET_ACCESS_KEY="your-secret"
./deploy.sh   # Interactive deployment
# Copy SQL examples from output and run in ClickHouse Cloud
```

**Technical Architecture:**
```
ClickHouse Cloud Service
    ‚Üì (AssumeRole)
IAM Role (ClickHouseS3Access)
    ‚Üì (S3 Permissions)
S3 Bucket (Encrypted, Versioned)
    ‚Üì (Parquet/CSV/JSON)
Your Data Files
```

---

## üõ† Prerequisites

### General Requirements
- macOS, Linux, or Windows with WSL2
- Docker and Docker Compose
- Basic command-line knowledge

### Specific Requirements by Lab
- **oss-mac-setup**: Docker Desktop for Mac
- **tpcds**: ClickHouse client, bash, sufficient disk space
- **datalake-minio-catalog**: Python 3.8+, 20GB+ disk space
- **terraform-minio-on-aws**: Terraform, AWS CLI, AWS account, EC2 key pair
- **terraform-glue-s3-chc-integration**: Terraform, AWS CLI, AWS account, ClickHouse Cloud account
- **terraform-chc-secures3-aws**: Terraform, AWS CLI, AWS account, ClickHouse Cloud account

## üöÄ Getting Started

1. **Clone this repository:**
   ```bash
   git clone https://github.com/yourusername/clickhouse-hols.git
   cd clickhouse-hols
   ```

2. **Choose a lab** from the list above based on your learning goals

3. **Follow the Quick Start** instructions in each lab's directory

4. **Read the detailed README** in each lab for comprehensive documentation

## üìñ Learning Path

**Recommended progression for beginners:**

1. **Start with [oss-mac-setup](oss-mac-setup/)** ‚Üí Learn basic ClickHouse operations locally
2. **Try [datalake-minio-catalog](datalake-minio-catalog/)** ‚Üí Understand data lake concepts and S3 integration
3. **Explore [tpcds](tpcds/)** ‚Üí Learn about ClickHouse performance and query optimization
4. **Deploy [terraform-minio-on-aws](terraform-minio-on-aws/)** ‚Üí Experience cloud infrastructure deployment
5. **Intermediate: [terraform-chc-secures3-aws](terraform-chc-secures3-aws/)** ‚Üí Secure S3 integration with ClickHouse Cloud
6. **Advanced: [terraform-glue-s3-chc-integration](terraform-glue-s3-chc-integration/)** ‚Üí Master ClickHouse Cloud + AWS Glue integration

## üá∞üá∑ ÌïúÍµ≠Ïñ¥ Ï†ïÎ≥¥ (Korean Information)

ClickHouseÏóê ÎåÄÌïú Îçî ÎßéÏùÄ Ï†ïÎ≥¥ÏôÄ ÌïúÍµ≠Ïñ¥ Î¶¨ÏÜåÏä§Îäî [clickhouse.kr](https://clickhouse.kr)ÏóêÏÑú ÌôïÏù∏ÌïòÏã§ Ïàò ÏûàÏäµÎãàÎã§.

If you are Korean, you can get more information from [clickhouse.kr](https://clickhouse.kr).

## ü§ù Contributing

Contributions are welcome! Please feel free to submit issues or pull requests.

## üìù License

MIT License - See individual lab directories for specific license information.

