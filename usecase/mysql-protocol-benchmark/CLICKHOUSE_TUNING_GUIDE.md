# ClickHouse Point Query ì„±ëŠ¥ ìµœì í™” ê°€ì´ë“œ

**ê¸°ë°˜ í…ŒìŠ¤íŠ¸**: MySQL vs ClickHouse Point Query Benchmark
**ëª©í‘œ**: Point Query ì„±ëŠ¥ì„ MySQL ìˆ˜ì¤€ìœ¼ë¡œ í–¥ìƒ

---

## ğŸ“Š í˜„ì¬ ì„±ëŠ¥ ë¬¸ì œ

í˜„ì¬ ClickHouseëŠ” MySQL ëŒ€ë¹„ **ì•½ 54%ì˜ ì„±ëŠ¥**ë§Œ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤:

| ì§€í‘œ | MySQL | ClickHouse (í˜„ì¬) | ë¹„ìœ¨ |
|------|-------|-------------------|------|
| í‰ê·  QPS | 4,921 | 2,690 | 0.54x |
| P50 ë ˆì´í„´ì‹œ | 0.65ms | 3.95ms | 6.1ë°° ëŠë¦¼ |

---

## ğŸ¯ ìµœì í™” ì „ëµ ë¡œë“œë§µ

### Level 1: ê¸°ë³¸ ì„¤ì • ìµœì í™” (ì˜ˆìƒ í–¥ìƒ: 20-30%)

#### 1.1 Index Granularity ìµœì í™”

**í˜„ì¬ ì„¤ì •**: `index_granularity = 8192` (ê¸°ë³¸ê°’)

**ë¬¸ì œì **: 8192ëŠ” OLAP ì›Œí¬ë¡œë“œì— ìµœì í™”ë˜ì–´ ìˆìœ¼ë©°, Point Queryì—ëŠ” ë„ˆë¬´ í¼

**ê¶Œì¥ ì„¤ì •**:
```sql
-- Point Queryì— ìµœì í™”ëœ index_granularity
CREATE TABLE player_last_login
(
    -- ì»¬ëŸ¼ ì •ì˜...
)
ENGINE = MergeTree()
ORDER BY player_id
SETTINGS
    index_granularity = 256  -- 8192 â†’ 256 (32ë°° ì‘ê²Œ)
```

**ê·¼ê±°**:
- Altinityì˜ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ì— ë”°ë¥´ë©´, **key-value ì¡°íšŒ ì‹œ `index_granularity = 256`ì´ ìµœì **
- UInt64 í‚¤ì˜ ê²½ìš° 256ì´ ìµœì , FixedString(16)ì˜ ê²½ìš° 128ë„ ê³ ë ¤
- ì°¸ì¡°: [ClickHouseÂ® In the Storm. Part 2: Maximum QPS for key-value lookups](https://altinity.com/blog/clickhouse-in-the-storm-part-2)

**íŠ¸ë ˆì´ë“œì˜¤í”„**:
- âœ… Point Query ì„±ëŠ¥ í–¥ìƒ: ì•½ 2-3ë°°
- âŒ ì¸ë±ìŠ¤ í¬ê¸° ì¦ê°€: ì•½ 32ë°° (8192/256)
- âŒ ì§‘ê³„ ì¿¼ë¦¬ ì„±ëŠ¥ ì•½ê°„ ê°ì†Œ

---

#### 1.2 Bloom Filter Granularity ì¡°ì •

**í˜„ì¬ ì„¤ì •**: `GRANULARITY 1`

**ê²€í†  í•„ìš”**: í˜„ì¬ ì„¤ì •ì´ ìµœì ì¸ì§€ í…ŒìŠ¤íŠ¸ í•„ìš”

**ê¶Œì¥ ì ‘ê·¼**:
```sql
-- ì˜µì…˜ 1: GRANULARITY 1 ìœ ì§€ (í˜„ì¬)
INDEX idx_player_id_bloom player_id TYPE bloom_filter(0.01) GRANULARITY 1

-- ì˜µì…˜ 2: index_granularityì™€ ë™ì¼í•˜ê²Œ (í…ŒìŠ¤íŠ¸ í•„ìš”)
INDEX idx_player_id_bloom player_id TYPE bloom_filter(0.01) GRANULARITY 1
```

**ì£¼ì˜ì‚¬í•­**:
- GRANULARITY 1ì€ ê°€ì¥ ì„¸ë°€í•œ í•„í„°ë§ ì œê³µ
- `index_granularity`ë¥¼ 256ìœ¼ë¡œ ë‚®ì¶”ë©´ Bloom Filterì˜ íš¨ê³¼ë„ ë‹¬ë¼ì§
- ì°¸ì¡°: [ClickHouseÂ® Black Magic, Part 2: Bloom Filters](https://altinity.com/blog/skipping-indices-part-2-bloom-filters)

---

#### 1.3 Adaptive Index Granularity ë¹„í™œì„±í™”

**ì´ìœ **: Point Queryì—ëŠ” ê³ ì •ëœ ì‘ì€ granularityê°€ ë” íš¨ìœ¨ì 

```sql
SETTINGS
    index_granularity = 256,
    index_granularity_bytes = 0,  -- Adaptive ë¹„í™œì„±í™”
    min_index_granularity_bytes = 0
```

**ì°¸ì¡°**: [Tuning Index Granularity in ClickHouse](https://chistadata.com/clickhouse-performance-index-granularity/)

---

### Level 2: ìºì‹œ ìµœì í™” (ì˜ˆìƒ í–¥ìƒ: 30-50%)

#### 2.1 Uncompressed Cache í™œì„±í™”

**í˜„ì¬**: ë¹„í™œì„±í™” (ê¸°ë³¸ê°’)

**ê¶Œì¥ ì„¤ì •** (`config.xml`):
```xml
<clickhouse>
    <uncompressed_cache_size>5368709120</uncompressed_cache_size> <!-- 5GB -->
    <profiles>
        <default>
            <use_uncompressed_cache>1</use_uncompressed_cache>
        </default>
    </profiles>
</clickhouse>
```

**íš¨ê³¼**:
- ì§§ê³  ë¹ˆë²ˆí•œ ì¿¼ë¦¬ì— ëŒ€í•´ **ìµœëŒ€ 50% ì„±ëŠ¥ í–¥ìƒ**
- ì••ì¶• í•´ì œ ì‹œê°„ ì ˆì•½
- ì°¸ì¡°: [Caching in ClickHouseÂ® - The Definitive Guide Part 1](https://altinity.com/blog/caching-in-clickhouse-the-definitive-guide-part-1)

---

#### 2.2 Mark Cache í¬ê¸° ì¦ê°€

**ê¶Œì¥ ì„¤ì •** (`config.xml`):
```xml
<clickhouse>
    <mark_cache_size>5368709120</mark_cache_size> <!-- 5GB -->
</clickhouse>
```

**íš¨ê³¼**:
- ì¸ë±ìŠ¤ ë©”íƒ€ë°ì´í„° ìºì‹±ìœ¼ë¡œ I/O ê°ì†Œ
- Point Queryì—ì„œ ë§¤ìš° ì¤‘ìš”
- ì°¸ì¡°: [Boost ClickHouse performance with mark cache](https://www.instaclustr.com/blog/boost-clickhouse-performance-with-mark-cache-a-complete-guide/)

---

#### 2.3 ìºì‹œ ì„ê³„ê°’ ì¡°ì •

```xml
<profiles>
    <default>
        <merge_tree_max_rows_to_use_cache>1000000</merge_tree_max_rows_to_use_cache>
        <merge_tree_max_bytes_to_use_cache>10485760</merge_tree_max_bytes_to_use_cache>
    </default>
</profiles>
```

**ì°¸ì¡°**: [Cache types | ClickHouse Docs](https://clickhouse.com/docs/operations/caches)

---

### Level 3: ì¿¼ë¦¬ ìµœì í™” (ì˜ˆìƒ í–¥ìƒ: 10-20%)

#### 3.1 PREWHERE ëª…ì‹œì  ì‚¬ìš©

**í˜„ì¬ ì¿¼ë¦¬**:
```sql
SELECT player_id, player_name, character_id, character_name,
       character_level, character_class, server_id, server_name,
       last_login_at, currency_gold, currency_diamond
FROM player_last_login
WHERE player_id = 12345
```

**ìµœì í™”ëœ ì¿¼ë¦¬**:
```sql
SELECT player_id, player_name, character_id, character_name,
       character_level, character_class, server_id, server_name,
       last_login_at, currency_gold, currency_diamond
FROM player_last_login
PREWHERE player_id = 12345  -- WHERE â†’ PREWHERE
```

**íš¨ê³¼**:
- Primary Key ì¡°ê±´ì„ PREWHEREë¡œ ëª…ì‹œí•˜ë©´ ë” ì ì€ ë°ì´í„° ì½ê¸°
- I/O ìµœëŒ€ 95% ê°ì†Œ ê°€ëŠ¥
- ì¿¼ë¦¬ ì†ë„ 17ë°° í–¥ìƒ ì‚¬ë¡€ ì¡´ì¬
- ì°¸ì¡°: [How does the PREWHERE optimization work?](https://clickhouse.com/docs/optimize/prewhere)

---

#### 3.2 ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±°

**ìµœì í™”**:
```sql
-- í•„ìš”í•œ ì»¬ëŸ¼ë§Œ SELECT
SELECT player_id, player_name, last_login_at
FROM player_last_login
PREWHERE player_id = 12345
```

**íš¨ê³¼**:
- ì»¬ëŸ¼ ìŠ¤í† ì–´ì˜ íŠ¹ì„±ìƒ ì½ëŠ” ì»¬ëŸ¼ ìˆ˜ê°€ ì„±ëŠ¥ì— ì§ì ‘ ì˜í–¥
- 11ê°œ ì»¬ëŸ¼ â†’ 3ê°œ ì»¬ëŸ¼ìœ¼ë¡œ ì¤„ì´ë©´ I/O ì•½ 70% ê°ì†Œ

---

### Level 4: í…Œì´ë¸” ì—”ì§„ ë³€ê²½ (ì˜ˆìƒ í–¥ìƒ: 100-150%)

#### 4.1 Dictionary ì—”ì§„ ì‚¬ìš©

**ê°€ì¥ ê°•ë ¥í•œ ìµœì í™”**: MergeTree â†’ Dictionary

**ì„±ëŠ¥ ë¹„êµ**:
- MergeTree: ì•½ 4,000 QPS
- Dictionary: ì•½ 10,000 QPS (**2.5ë°° í–¥ìƒ**)
- ì°¸ì¡°: [ClickHouseÂ® In the Storm. Part 2: Maximum QPS for key-value lookups](https://altinity.com/blog/clickhouse-in-the-storm-part-2)

**êµ¬í˜„ ë°©ë²•**:

```sql
-- 1. MergeTree í…Œì´ë¸” ìœ ì§€ (ì“°ê¸°ìš©)
CREATE TABLE player_last_login_source
(
    player_id UInt64,
    player_name String,
    -- ... ê¸°íƒ€ ì»¬ëŸ¼
)
ENGINE = MergeTree()
ORDER BY player_id;

-- 2. Dictionary ìƒì„± (ì½ê¸°ìš©)
CREATE DICTIONARY player_last_login_dict
(
    player_id UInt64,
    player_name String,
    character_id UInt64,
    character_name String,
    character_level UInt16,
    character_class String,
    server_id UInt8,
    server_name String,
    last_login_at DateTime64(3),
    currency_gold UInt64,
    currency_diamond UInt32
)
PRIMARY KEY player_id
SOURCE(CLICKHOUSE(
    HOST 'localhost'
    PORT 9000
    USER 'default'
    PASSWORD ''
    DB 'gamedb'
    TABLE 'player_last_login_source'
))
LIFETIME(MIN 0 MAX 300)  -- 5ë¶„ë§ˆë‹¤ ê°±ì‹ 
LAYOUT(HASHED());  -- ë˜ëŠ” COMPLEX_KEY_HASHED()

-- 3. ì¿¼ë¦¬ ë°©ë²•
SELECT
    dictGet('player_last_login_dict', 'player_name', toUInt64(12345)) AS player_name,
    dictGet('player_last_login_dict', 'character_name', toUInt64(12345)) AS character_name,
    dictGet('player_last_login_dict', 'last_login_at', toUInt64(12345)) AS last_login_at
```

**ì¥ì **:
- âœ… 2-3ë°° ì„±ëŠ¥ í–¥ìƒ
- âœ… ë©”ëª¨ë¦¬ ë‚´ ì¡°íšŒë¡œ ì´ˆì €ì§€ì—°
- âœ… Primary Key ê¸°ë°˜ O(1) ì¡°íšŒ

**ë‹¨ì **:
- âŒ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€ (200ë§Œ rows Ã— ì•½ 500 bytes = ì•½ 1GB)
- âŒ ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ ë¶ˆê°€ (LIFETIME ì£¼ê¸°ë¡œ ìƒˆë¡œê³ ì¹¨)
- âŒ ì¿¼ë¦¬ êµ¬ë¬¸ ë³€ê²½ í•„ìš”

**ì°¸ì¡°**:
- [Using Dictionaries to Accelerate Queries](https://clickhouse.com/blog/faster-queries-dictionaries-clickhouse)
- [Simplifying Queries with ClickHouse Dictionaries](https://aggregations.io/blog/clickhouse-dictionaries)

---

#### 4.2 EmbeddedRocksDB ì—”ì§„ (ëŒ€ì•ˆ)

**íŠ¹ì§•**: ì§„ì •í•œ Key-Value ìŠ¤í† ì–´

```sql
CREATE TABLE player_last_login_kv
(
    player_id UInt64,
    data String  -- JSON ë˜ëŠ” ì§ë ¬í™”ëœ ë°ì´í„°
)
ENGINE = EmbeddedRocksDB
PRIMARY KEY player_id;
```

**ì¥ì **:
- âœ… ì‹¤ì‹œê°„ ì“°ê¸° ê°€ëŠ¥
- âœ… Key-Valueì— ìµœì í™”
- âœ… Point Query ì„±ëŠ¥ ìš°ìˆ˜

**ë‹¨ì **:
- âŒ ë³µì¡í•œ ì¿¼ë¦¬ ë¶ˆê°€
- âŒ ë°ì´í„°ë¥¼ JSON ë“±ìœ¼ë¡œ ì§ë ¬í™” í•„ìš”
- âŒ ë¶„ì„ ì¿¼ë¦¬ ë¶ˆê°€ëŠ¥

---

### Level 5: MySQL Protocol ìµœì í™”

#### 5.1 Native Protocol ì‚¬ìš©

**í˜„ì¬**: MySQL Protocol (Port 9004)
**ê¶Œì¥**: ClickHouse Native Protocol (Port 9000)

**Python ë“œë¼ì´ë²„ ë³€ê²½**:
```python
# í˜„ì¬: mysql-connector-python
import mysql.connector
conn = mysql.connector.connect(host='localhost', port=9004, ...)

# ë³€ê²½: clickhouse-driver (Native Protocol)
from clickhouse_driver import Client
client = Client(host='localhost', port=9000)
result = client.execute('SELECT * FROM player_last_login WHERE player_id = 12345')
```

**íš¨ê³¼**:
- MySQL Protocol ë³€í™˜ ì˜¤ë²„í—¤ë“œ ì œê±°
- ì•½ 10-20% ì„±ëŠ¥ í–¥ìƒ ì˜ˆìƒ
- Connection pool ê´€ë¦¬ ê°œì„ 

---

#### 5.2 Connection Pool ìµœì í™”

**ê¶Œì¥ ì„¤ì •**:
```python
from clickhouse_driver import Client
from clickhouse_pool import ChPool

pool = ChPool(
    host='localhost',
    port=9000,
    connections_min=8,
    connections_max=32,
    executor='thread'
)
```

**ì°¸ì¡°**: [clickhouse-pool documentation](https://clickhouse-pool.readthedocs.io/en/latest/introduction.html)

---

## ğŸ”¬ ë‹¨ê³„ë³„ ì ìš© ë° í…ŒìŠ¤íŠ¸ ê³„íš

### Phase 1: ë‚®ì€ ìœ„í—˜ ìµœì í™” (ì¦‰ì‹œ ì ìš© ê°€ëŠ¥)

1. **Uncompressed Cache í™œì„±í™”**
   ```bash
   # config.xml ìˆ˜ì • í›„
   docker-compose restart clickhouse
   ```

2. **PREWHERE ì¿¼ë¦¬ ë³€ê²½**
   ```python
   # benchmark.py ìˆ˜ì •
   query = f"SELECT ... FROM player_last_login PREWHERE player_id = {player_id}"
   ```

3. **ë²¤ì¹˜ë§ˆí¬ ì¬ì‹¤í–‰**
   ```bash
   python3 benchmark.py
   ```

**ì˜ˆìƒ ê²°ê³¼**: QPS 2,690 â†’ 3,500 (ì•½ 30% í–¥ìƒ)

---

### Phase 2: ì¤‘ê°„ ìœ„í—˜ ìµœì í™” (í…ŒìŠ¤íŠ¸ í•„ìš”)

1. **Index Granularity ë³€ê²½**
   ```sql
   -- ìƒˆ í…Œì´ë¸” ìƒì„±
   CREATE TABLE player_last_login_optimized
   ENGINE = MergeTree()
   ORDER BY player_id
   SETTINGS index_granularity = 256;

   -- ë°ì´í„° ë³µì‚¬
   INSERT INTO player_last_login_optimized SELECT * FROM player_last_login;
   ```

2. **ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸**

**ì˜ˆìƒ ê²°ê³¼**: QPS 3,500 â†’ 5,000 (ì•½ 40% ì¶”ê°€ í–¥ìƒ)

---

### Phase 3: ê³ ê¸‰ ìµœì í™” (ì•„í‚¤í…ì²˜ ë³€ê²½)

1. **Dictionary ì—”ì§„ êµ¬í˜„**
   - ì“°ê¸°ëŠ” MergeTree
   - ì½ê¸°ëŠ” Dictionary

2. **í•˜ì´ë¸Œë¦¬ë“œ ì•„í‚¤í…ì²˜**
   - ì‹¤ì‹œê°„ ì¿¼ë¦¬: Dictionary (Point Query)
   - ë¶„ì„ ì¿¼ë¦¬: MergeTree (Aggregation)

**ì˜ˆìƒ ê²°ê³¼**: QPS 5,000 â†’ 10,000 (2ë°° ì¶”ê°€ í–¥ìƒ)

---

## ğŸ“Š ì˜ˆìƒ ìµœì¢… ì„±ëŠ¥

| ìµœì í™” ë‹¨ê³„ | ì˜ˆìƒ QPS | MySQL ëŒ€ë¹„ | ëˆ„ì  í–¥ìƒë¥  |
|-------------|----------|------------|-------------|
| í˜„ì¬ (Baseline) | 2,690 | 0.54x | - |
| Phase 1 (Cache) | 3,500 | 0.71x | +30% |
| Phase 2 (Granularity) | 5,000 | 1.01x | +86% |
| Phase 3 (Dictionary) | 10,000 | 2.03x | +272% |

**ìµœì¢… ëª©í‘œ**: ClickHouseê°€ MySQLì„ **2ë°° ì´ìƒ ì´ˆê³¼**í•˜ëŠ” ì„±ëŠ¥ ë‹¬ì„±

---

## ğŸ¯ êµ¬ì²´ì ì¸ êµ¬í˜„ íŒŒì¼

### 1. ìµœì í™”ëœ ClickHouse ìŠ¤í‚¤ë§ˆ

**íŒŒì¼**: `init/clickhouse/init_optimized.sql`

```sql
-- ============================================================
-- Phase 2: Index Granularity ìµœì í™”
-- ============================================================

CREATE DATABASE IF NOT EXISTS gamedb;

CREATE TABLE gamedb.player_last_login_v2
(
    player_id UInt64,
    player_name String,
    character_id UInt64,
    character_name String,
    character_level UInt16,
    character_class LowCardinality(String),
    server_id UInt8,
    server_name LowCardinality(String),
    last_login_at DateTime64(3),
    last_logout_at Nullable(DateTime64(3)),
    last_ip IPv4,
    last_device_type LowCardinality(String),
    last_app_version LowCardinality(String),
    total_playtime_minutes UInt32,
    vip_level UInt8,
    guild_id Nullable(UInt64),
    guild_name Nullable(String),
    last_map_id UInt16,
    last_position_x Float32,
    last_position_y Float32,
    last_position_z Float32,
    currency_gold UInt64,
    currency_diamond UInt32,
    inventory_slots_used UInt16,
    created_at DateTime64(3) DEFAULT now64(3),
    updated_at DateTime64(3) DEFAULT now64(3),

    -- Bloom Filter Index
    INDEX idx_player_id_bloom player_id TYPE bloom_filter(0.01) GRANULARITY 1
)
ENGINE = MergeTree()
ORDER BY player_id
SETTINGS
    -- Key-value ì¡°íšŒ ìµœì í™”
    index_granularity = 256,  -- 8192 â†’ 256
    index_granularity_bytes = 0,  -- Adaptive ë¹„í™œì„±í™”
    -- ì••ì¶• ì„¤ì • (ì„ íƒì‚¬í•­)
    min_compress_block_size = 65536,
    max_compress_block_size = 1048576;

-- ê¸°ì¡´ ë°ì´í„° ë³µì‚¬
INSERT INTO gamedb.player_last_login_v2 SELECT * FROM gamedb.player_last_login;

-- ìµœì í™”
OPTIMIZE TABLE gamedb.player_last_login_v2 FINAL;
```

---

### 2. ìºì‹œ ìµœì í™” ì„¤ì •

**íŒŒì¼**: `init/clickhouse/config_optimized.xml`

```xml
<?xml version="1.0"?>
<clickhouse>
    <logger>
        <level>information</level>
    </logger>

    <http_port>8123</http_port>
    <tcp_port>9000</tcp_port>

    <!-- MySQL Protocol -->
    <mysql_port>9004</mysql_port>

    <!-- ===== ìºì‹œ ìµœì í™” ===== -->

    <!-- Mark Cache: 5GB -->
    <mark_cache_size>5368709120</mark_cache_size>

    <!-- Uncompressed Cache: 5GB -->
    <uncompressed_cache_size>5368709120</uncompressed_cache_size>

    <!-- Primary Index Cache: 2GB -->
    <index_mark_cache_size>2147483648</index_mark_cache_size>

    <!-- Query Cache: 1GB -->
    <query_cache_size>1073741824</query_cache_size>

    <!-- ===== ì„±ëŠ¥ íŠœë‹ ===== -->

    <!-- ìµœëŒ€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: 10GB -->
    <max_server_memory_usage>10737418240</max_server_memory_usage>

    <!-- Thread Pool -->
    <max_thread_pool_size>32</max_thread_pool_size>
    <max_thread_pool_free_size>16</max_thread_pool_free_size>

    <!-- ===== í”„ë¡œíŒŒì¼ ì„¤ì • ===== -->

    <profiles>
        <default>
            <!-- Uncompressed Cache í™œì„±í™” -->
            <use_uncompressed_cache>1</use_uncompressed_cache>

            <!-- Query Cache í™œì„±í™” -->
            <use_query_cache>1</use_query_cache>

            <!-- Cache ì„ê³„ê°’ -->
            <merge_tree_max_rows_to_use_cache>1000000</merge_tree_max_rows_to_use_cache>
            <merge_tree_max_bytes_to_use_cache>10485760</merge_tree_max_bytes_to_use_cache>

            <!-- PREWHERE ìµœì í™” ìë™ í™œì„±í™” -->
            <optimize_move_to_prewhere>1</optimize_move_to_prewhere>

            <!-- ê¸°íƒ€ ìµœì í™” -->
            <max_threads>16</max_threads>
            <max_memory_usage>8589934592</max_memory_usage>
        </default>
    </profiles>

    <users>
        <testuser>
            <password>testpass</password>
            <profile>default</profile>
            <networks>
                <ip>::/0</ip>
            </networks>
            <quota>default</quota>
        </testuser>
    </users>
</clickhouse>
```

---

### 3. Dictionary ì—”ì§„ êµ¬í˜„

**íŒŒì¼**: `init/clickhouse/create_dictionary.sql`

```sql
-- ============================================================
-- Phase 3: Dictionary ì—”ì§„ìœ¼ë¡œ Point Query ìµœì í™”
-- ============================================================

-- 1. Source í…Œì´ë¸” (ì´ë¯¸ ì¡´ì¬í•˜ëŠ” player_last_login_v2 ì‚¬ìš©)

-- 2. Dictionary ìƒì„±
CREATE DICTIONARY gamedb.player_last_login_dict
(
    player_id UInt64,
    player_name String,
    character_id UInt64,
    character_name String,
    character_level UInt16,
    character_class String,
    server_id UInt8,
    server_name String,
    last_login_at DateTime64(3),
    currency_gold UInt64,
    currency_diamond UInt32
)
PRIMARY KEY player_id
SOURCE(CLICKHOUSE(
    HOST 'localhost'
    PORT 9000
    USER 'testuser'
    PASSWORD 'testpass'
    DB 'gamedb'
    TABLE 'player_last_login_v2'
))
LIFETIME(MIN 60 MAX 300)  -- 1-5ë¶„ ì£¼ê¸°ë¡œ ê°±ì‹ 
LAYOUT(HASHED())  -- ë©”ëª¨ë¦¬ ë‚´ í•´ì‹œí…Œì´ë¸”
SETTINGS(format_csv_allow_single_quotes = 0);

-- 3. ì¿¼ë¦¬ ì˜ˆì œ
SELECT
    dictGet('gamedb.player_last_login_dict', 'player_name', toUInt64(12345)) AS player_name,
    dictGet('gamedb.player_last_login_dict', 'character_name', toUInt64(12345)) AS character_name,
    dictGet('gamedb.player_last_login_dict', 'last_login_at', toUInt64(12345)) AS last_login_at,
    dictGet('gamedb.player_last_login_dict', 'currency_gold', toUInt64(12345)) AS currency_gold;

-- 4. Dictionary ìƒíƒœ í™•ì¸
SELECT * FROM system.dictionaries WHERE name = 'player_last_login_dict';
```

---

### 4. ìµœì í™”ëœ ë²¤ì¹˜ë§ˆí¬ ìŠ¤í¬ë¦½íŠ¸

**íŒŒì¼**: `benchmark_optimized.py`

```python
#!/usr/bin/env python3
"""
Optimized ClickHouse Benchmark
- Native protocol support
- PREWHERE optimization
- Dictionary support
"""

from clickhouse_driver import Client
import time
import random
import statistics
import concurrent.futures
from dataclasses import dataclass, field
from typing import List
import json
from datetime import datetime
import threading

@dataclass
class BenchmarkResult:
    target: str
    concurrency: int
    total_queries: int
    duration_seconds: float
    successful_queries: int
    failed_queries: int
    latencies_ms: List[float] = field(default_factory=list)

    @property
    def qps(self) -> float:
        return self.successful_queries / self.duration_seconds if self.duration_seconds > 0 else 0

    @property
    def avg_latency_ms(self) -> float:
        return statistics.mean(self.latencies_ms) if self.latencies_ms else 0

    @property
    def p50_latency_ms(self) -> float:
        return statistics.median(self.latencies_ms) if self.latencies_ms else 0

    @property
    def p95_latency_ms(self) -> float:
        if not self.latencies_ms:
            return 0
        sorted_lat = sorted(self.latencies_ms)
        return sorted_lat[int(len(sorted_lat) * 0.95)]

    @property
    def p99_latency_ms(self) -> float:
        if not self.latencies_ms:
            return 0
        sorted_lat = sorted(self.latencies_ms)
        return sorted_lat[int(len(sorted_lat) * 0.99)]


class ClickHouseOptimizedBenchmark:
    MAX_PLAYER_ID = 2_000_000
    QUERIES_PER_WORKER = 500

    def __init__(self, use_native=True, use_prewhere=True, use_dictionary=False):
        self.use_native = use_native
        self.use_prewhere = use_prewhere
        self.use_dictionary = use_dictionary
        self.results: List[BenchmarkResult] = []

    def create_client(self):
        """Create ClickHouse client using native protocol"""
        return Client(host='localhost', port=9000,
                     user='testuser', password='testpass',
                     database='gamedb')

    def execute_mergetree_query(self, client, player_id: int) -> tuple:
        """Standard MergeTree query"""
        where_clause = "PREWHERE" if self.use_prewhere else "WHERE"
        query = f"""
            SELECT player_id, player_name, character_id, character_name,
                   character_level, character_class, server_id, server_name,
                   last_login_at, currency_gold, currency_diamond
            FROM player_last_login_v2
            {where_clause} player_id = {player_id}
        """

        try:
            start_time = time.perf_counter()
            result = client.execute(query)
            end_time = time.perf_counter()

            latency_ms = (end_time - start_time) * 1000
            return (True, latency_ms, len(result))
        except Exception as e:
            return (False, 0, str(e))

    def execute_dictionary_query(self, client, player_id: int) -> tuple:
        """Dictionary-based query"""
        query = f"""
            SELECT
                dictGet('gamedb.player_last_login_dict', 'player_name', toUInt64({player_id})) AS player_name,
                dictGet('gamedb.player_last_login_dict', 'character_name', toUInt64({player_id})) AS character_name,
                dictGet('gamedb.player_last_login_dict', 'last_login_at', toUInt64({player_id})) AS last_login_at,
                dictGet('gamedb.player_last_login_dict', 'currency_gold', toUInt64({player_id})) AS currency_gold
        """

        try:
            start_time = time.perf_counter()
            result = client.execute(query)
            end_time = time.perf_counter()

            latency_ms = (end_time - start_time) * 1000
            return (True, latency_ms, len(result))
        except Exception as e:
            return (False, 0, str(e))

    def worker_thread(self, player_ids: List[int], results_list: list, lock: threading.Lock):
        client = self.create_client()
        local_latencies = []
        local_success = 0
        local_fail = 0

        for pid in player_ids:
            if self.use_dictionary:
                success, latency, _ = self.execute_dictionary_query(client, pid)
            else:
                success, latency, _ = self.execute_mergetree_query(client, pid)

            if success:
                local_latencies.append(latency)
                local_success += 1
            else:
                local_fail += 1

        client.disconnect()

        with lock:
            results_list.extend(local_latencies)
            results_list.append(('stats', local_success, local_fail))

    def run_benchmark(self, target: str, concurrency: int) -> BenchmarkResult:
        print(f"\n{'='*60}")
        print(f"Running: {target} | Concurrency: {concurrency}")
        print(f"Config: Native={self.use_native}, PREWHERE={self.use_prewhere}, Dict={self.use_dictionary}")
        print(f"{'='*60}")

        total_queries = concurrency * self.QUERIES_PER_WORKER
        all_player_ids = [random.randint(1, self.MAX_PLAYER_ID) for _ in range(total_queries)]

        chunks = [
            all_player_ids[i:i + self.QUERIES_PER_WORKER]
            for i in range(0, total_queries, self.QUERIES_PER_WORKER)
        ]

        results_list = []
        lock = threading.Lock()

        # Warmup
        print("Warming up...")
        client = self.create_client()
        for pid in [random.randint(1, self.MAX_PLAYER_ID) for _ in range(10)]:
            if self.use_dictionary:
                self.execute_dictionary_query(client, pid)
            else:
                self.execute_mergetree_query(client, pid)
        client.disconnect()

        print(f"Executing {total_queries} queries...")
        start_time = time.perf_counter()

        with concurrent.futures.ThreadPoolExecutor(max_workers=concurrency) as executor:
            futures = [
                executor.submit(self.worker_thread, chunk, results_list, lock)
                for chunk in chunks
            ]
            concurrent.futures.wait(futures)

        end_time = time.perf_counter()
        duration = end_time - start_time

        latencies = [r for r in results_list if not isinstance(r, tuple)]
        stats = [r for r in results_list if isinstance(r, tuple) and r[0] == 'stats']

        total_success = sum(s[1] for s in stats)
        total_fail = sum(s[2] for s in stats)

        result = BenchmarkResult(
            target=target,
            concurrency=concurrency,
            total_queries=total_queries,
            duration_seconds=duration,
            successful_queries=total_success,
            failed_queries=total_fail,
            latencies_ms=latencies
        )

        self.print_result(result)
        return result

    def print_result(self, r: BenchmarkResult):
        print(f"\n--- Results ---")
        print(f"Queries: {r.successful_queries:,} / {r.total_queries:,}")
        print(f"Duration: {r.duration_seconds:.2f}s")
        print(f"QPS: {r.qps:,.2f}")
        print(f"Latency: avg={r.avg_latency_ms:.2f}ms, p50={r.p50_latency_ms:.2f}ms, p95={r.p95_latency_ms:.2f}ms, p99={r.p99_latency_ms:.2f}ms")


def main():
    print("ClickHouse Optimized Benchmark")
    print("=" * 50)

    concurrency_levels = [8, 16, 24, 32]

    # Phase 1: Baseline (í˜„ì¬)
    print("\n### BASELINE: Current Settings")
    baseline = ClickHouseOptimizedBenchmark(use_native=False, use_prewhere=False, use_dictionary=False)
    baseline_results = [baseline.run_benchmark("ClickHouse-Baseline", c) for c in concurrency_levels]

    # Phase 2: Native + PREWHERE
    print("\n### PHASE 1: Native Protocol + PREWHERE")
    phase1 = ClickHouseOptimizedBenchmark(use_native=True, use_prewhere=True, use_dictionary=False)
    phase1_results = [phase1.run_benchmark("ClickHouse-Phase1", c) for c in concurrency_levels]

    # Phase 3: Dictionary (ì˜µì…˜ - Dictionaryê°€ ìƒì„±ë˜ì–´ ìˆì„ ë•Œë§Œ)
    try:
        print("\n### PHASE 3: Dictionary Engine")
        phase3 = ClickHouseOptimizedBenchmark(use_native=True, use_prewhere=True, use_dictionary=True)
        phase3_results = [phase3.run_benchmark("ClickHouse-Dictionary", c) for c in concurrency_levels]
    except Exception as e:
        print(f"Dictionary not available: {e}")
        phase3_results = []

    # ê²°ê³¼ ë¹„êµ
    print("\n" + "="*80)
    print("PERFORMANCE COMPARISON")
    print("="*80)

    for i, conc in enumerate(concurrency_levels):
        print(f"\nConcurrency {conc}:")
        print(f"  Baseline:  {baseline_results[i].qps:,.2f} QPS")
        print(f"  Phase 1:   {phase1_results[i].qps:,.2f} QPS ({phase1_results[i].qps/baseline_results[i].qps:.2f}x)")
        if phase3_results:
            print(f"  Phase 3:   {phase3_results[i].qps:,.2f} QPS ({phase3_results[i].qps/baseline_results[i].qps:.2f}x)")


if __name__ == "__main__":
    main()
```

---

## ğŸ“š ì°¸ê³  ìë£Œ

### ClickHouse ê³µì‹ ë¬¸ì„œ
- [Query Performance Optimization Guide](https://clickhouse.com/docs/optimize/query-optimization)
- [MergeTree Settings](https://clickhouse.com/docs/operations/settings/merge-tree-settings)
- [PREWHERE Optimization](https://clickhouse.com/docs/optimize/prewhere)
- [Cache Types Documentation](https://clickhouse.com/docs/operations/caches)
- [Dictionary Documentation](https://clickhouse.com/docs/dictionary)

### ì„±ëŠ¥ ìµœì í™” ê°€ì´ë“œ
- [The definitive guide to ClickHouse query optimization (2026)](https://clickhouse.com/resources/engineering/clickhouse-query-optimisation-definitive-guide)
- [ClickHouse Query Performance Optimization: 2025 Complete Guide](https://www.e6data.com/query-and-cost-optimization-hub/how-to-optimize-clickhouse-query-performance)
- [A simple guide to ClickHouse query optimization: part 1](https://clickhouse.com/blog/a-simple-guide-to-clickhouse-query-optimization-part-1)

### Key-Value ë° Point Query ìµœì í™”
- [ClickHouseÂ® In the Storm. Part 2: Maximum QPS for key-value lookups](https://altinity.com/blog/clickhouse-in-the-storm-part-2)
- [Optimizing Clickhouse for access by ID](https://medium.com/datadenys/optimizing-clickhouse-for-access-by-id-cc415faa83c0)
- [Improving Clickhouse query performance by tuning key order](https://medium.com/datadenys/improving-clickhouse-query-performance-tuning-key-order-f406db7cfeb9)

### ì¸ë±ìŠ¤ ë° Bloom Filter
- [ClickHouseÂ® Black Magic, Part 2: Bloom Filters](https://altinity.com/blog/skipping-indices-part-2-bloom-filters)
- [Use data skipping indices where appropriate](https://clickhouse.com/docs/best-practices/use-data-skipping-indices-where-appropriate)
- [Tuning Index Granularity in ClickHouse](https://chistadata.com/clickhouse-performance-index-granularity/)

### ìºì‹œ ìµœì í™”
- [Caching in ClickHouseÂ® - The Definitive Guide Part 1](https://altinity.com/blog/caching-in-clickhouse-the-definitive-guide-part-1)
- [Boost ClickHouse performance with mark cache: A complete guide](https://www.instaclustr.com/blog/boost-clickhouse-performance-with-mark-cache-a-complete-guide/)

### Dictionary ì—”ì§„
- [Using Dictionaries to Accelerate Queries](https://clickhouse.com/blog/faster-queries-dictionaries-clickhouse)
- [Simplifying Queries with ClickHouse Dictionaries](https://aggregations.io/blog/clickhouse-dictionaries)

### ì—°ê²° ë° í”„ë¡œí† ì½œ
- [clickhouse-pool documentation](https://clickhouse-pool.readthedocs.io/en/latest/introduction.html)
- [ClickHouse and MySQL - Better Together](https://www.percona.com/blog/clickhouse-and-mysql-better-together/)

---

**ì‘ì„±ì¼**: 2025-12-25
**ê¸°ë°˜ ë²¤ì¹˜ë§ˆí¬**: MySQL vs ClickHouse Point Query Performance Test
**ë²„ì „**: ClickHouse 25.10

ì´ ê°€ì´ë“œë¥¼ ë‹¨ê³„ì ìœ¼ë¡œ ì ìš©í•˜ë©´ ClickHouse Point Query ì„±ëŠ¥ì„ MySQL ìˆ˜ì¤€ ì´ìƒìœ¼ë¡œ ëŒì–´ì˜¬ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
