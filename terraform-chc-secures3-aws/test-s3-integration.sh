#!/bin/bash

# ClickHouse Cloud S3 Integration Test Script
# This script tests the S3 table engine integration with ClickHouse Cloud

set -e

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Function to print colored messages
print_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

print_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

print_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

print_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# Check if terraform is installed
if ! command -v terraform &> /dev/null; then
    print_error "Terraform is not installed. Please install Terraform first."
    exit 1
fi

# Check if AWS CLI is installed
if ! command -v aws &> /dev/null; then
    print_error "AWS CLI is not installed. Please install AWS CLI first."
    exit 1
fi

print_info "Starting ClickHouse S3 Integration Test"
echo ""

# Get outputs from Terraform
print_info "Retrieving Terraform outputs..."
BUCKET_NAME=$(terraform output -raw bucket_name 2>/dev/null)
IAM_ROLE_ARN=$(terraform output -raw iam_role_arn 2>/dev/null)
BUCKET_REGION=$(terraform output -raw bucket_region 2>/dev/null)

if [ -z "$BUCKET_NAME" ] || [ -z "$IAM_ROLE_ARN" ] || [ -z "$BUCKET_REGION" ]; then
    print_error "Failed to retrieve Terraform outputs. Have you run 'terraform apply'?"
    exit 1
fi

print_success "Retrieved Terraform outputs:"
echo "  Bucket Name: $BUCKET_NAME"
echo "  IAM Role ARN: $IAM_ROLE_ARN"
echo "  Bucket Region: $BUCKET_REGION"
echo ""

# Test 1: Verify S3 bucket exists
print_info "Test 1: Verifying S3 bucket exists..."
if aws s3 ls "s3://${BUCKET_NAME}" &>/dev/null; then
    print_success "S3 bucket exists and is accessible"
else
    print_error "Cannot access S3 bucket. Check AWS credentials and permissions."
    exit 1
fi
echo ""

# Test 2: Upload a test file to S3
print_info "Test 2: Uploading test data to S3..."
TEST_FILE="/tmp/clickhouse_test_data.csv"
cat > "$TEST_FILE" << EOF
id,name,value,timestamp
1,Alice,100,2024-01-01 10:00:00
2,Bob,200,2024-01-01 11:00:00
3,Charlie,300,2024-01-01 12:00:00
4,Diana,400,2024-01-01 13:00:00
5,Eve,500,2024-01-01 14:00:00
EOF

if aws s3 cp "$TEST_FILE" "s3://${BUCKET_NAME}/test/data.csv" &>/dev/null; then
    print_success "Test file uploaded successfully"
    rm "$TEST_FILE"
else
    print_error "Failed to upload test file to S3"
    exit 1
fi
echo ""

# Test 3: Generate SQL for ClickHouse
print_info "Test 3: Generating ClickHouse SQL scripts..."

SQL_FILE="test_s3_queries.sql"
cat > "$SQL_FILE" << EOF
-- ClickHouse S3 Integration Test Queries
-- Generated by test-s3-integration.sh

-- ==============================================================================
-- Test 1: Create S3-backed table with CSV data
-- ==============================================================================
CREATE TABLE IF NOT EXISTS test_s3_table
(
    id UInt64,
    name String,
    value UInt64,
    timestamp DateTime
)
ENGINE = S3(
    'https://s3.${BUCKET_REGION}.amazonaws.com/${BUCKET_NAME}/test/data.csv',
    'CSVWithNames',
    extra_credentials(role_arn = '${IAM_ROLE_ARN}')
);

-- ==============================================================================
-- Test 2: Query data from S3
-- ==============================================================================
SELECT 'Test Query: All data from S3' AS test;
SELECT * FROM test_s3_table;

-- ==============================================================================
-- Test 3: Aggregate query on S3 data
-- ==============================================================================
SELECT 'Test Query: Aggregation' AS test;
SELECT
    count() AS total_rows,
    sum(value) AS total_value,
    avg(value) AS avg_value,
    min(timestamp) AS min_time,
    max(timestamp) AS max_time
FROM test_s3_table;

-- ==============================================================================
-- Test 4: Insert data to S3 (Parquet format)
-- ==============================================================================
SELECT 'Test Query: Insert to S3 Parquet' AS test;
CREATE TABLE IF NOT EXISTS test_s3_output
(
    id UInt64,
    name String,
    value UInt64,
    timestamp DateTime,
    processed_at DateTime DEFAULT now()
)
ENGINE = S3(
    'https://s3.${BUCKET_REGION}.amazonaws.com/${BUCKET_NAME}/test/output.parquet',
    'Parquet',
    extra_credentials(role_arn = '${IAM_ROLE_ARN}')
);

-- Insert processed data
INSERT INTO test_s3_output (id, name, value, timestamp)
SELECT id, name, value * 2 AS value, timestamp
FROM test_s3_table;

-- ==============================================================================
-- Test 5: Read back the inserted data
-- ==============================================================================
SELECT 'Test Query: Read inserted data' AS test;
SELECT * FROM test_s3_output;

-- ==============================================================================
-- Test 6: Direct S3 query without creating table
-- ==============================================================================
SELECT 'Test Query: Direct S3 query' AS test;
SELECT count() AS row_count
FROM s3(
    'https://s3.${BUCKET_REGION}.amazonaws.com/${BUCKET_NAME}/test/*.csv',
    'CSVWithNames',
    extra_credentials(role_arn = '${IAM_ROLE_ARN}')
);

-- ==============================================================================
-- Test 7: Export aggregated results to S3
-- ==============================================================================
SELECT 'Test Query: Export aggregation to S3' AS test;
INSERT INTO FUNCTION s3(
    'https://s3.${BUCKET_REGION}.amazonaws.com/${BUCKET_NAME}/test/aggregated_results.parquet',
    'Parquet',
    'name String, total_value UInt64',
    extra_credentials(role_arn = '${IAM_ROLE_ARN}')
)
SELECT
    name,
    sum(value) AS total_value
FROM test_s3_table
GROUP BY name;

-- ==============================================================================
-- Test 8: Verify exported data
-- ==============================================================================
SELECT 'Test Query: Verify exported data' AS test;
SELECT *
FROM s3(
    'https://s3.${BUCKET_REGION}.amazonaws.com/${BUCKET_NAME}/test/aggregated_results.parquet',
    'Parquet',
    extra_credentials(role_arn = '${IAM_ROLE_ARN}')
);

-- ==============================================================================
-- Cleanup (optional - comment out to keep test data)
-- ==============================================================================
-- DROP TABLE IF EXISTS test_s3_table;
-- DROP TABLE IF EXISTS test_s3_output;
EOF

print_success "SQL test file generated: $SQL_FILE"
echo ""

# Test 4: Verify files in S3
print_info "Test 4: Listing files in S3 test directory..."
if aws s3 ls "s3://${BUCKET_NAME}/test/" | grep -q "data.csv"; then
    print_success "Test files confirmed in S3:"
    aws s3 ls "s3://${BUCKET_NAME}/test/" | sed 's/^/  /'
else
    print_warning "Test directory listing is empty or inaccessible"
fi
echo ""

# Test 5: Generate example SQL files
print_info "Test 5: Generating additional SQL examples..."

# Example 1: Logs table
cat > "example_logs_table.sql" << EOF
-- Example: Log data with S3 engine (Parquet format)
CREATE TABLE logs_s3
(
    timestamp DateTime,
    level String,
    message String,
    service String,
    host String
)
ENGINE = S3(
    'https://s3.${BUCKET_REGION}.amazonaws.com/${BUCKET_NAME}/logs/app_logs.parquet',
    'Parquet',
    extra_credentials(role_arn = '${IAM_ROLE_ARN}')
);

-- Insert sample log data
INSERT INTO logs_s3 VALUES
    (now(), 'INFO', 'Application started', 'web-api', 'host-1'),
    (now(), 'DEBUG', 'Processing request', 'web-api', 'host-1'),
    (now(), 'ERROR', 'Connection timeout', 'web-api', 'host-2'),
    (now(), 'INFO', 'Request completed', 'web-api', 'host-1');

-- Query logs
SELECT * FROM logs_s3 WHERE level = 'ERROR';
EOF

# Example 2: Events table
cat > "example_events_table.sql" << EOF
-- Example: Event tracking with S3 engine (JSON format)
CREATE TABLE events_s3
(
    event_id UInt64,
    user_id String,
    event_type String,
    event_data String,
    created_at DateTime DEFAULT now()
)
ENGINE = S3(
    'https://s3.${BUCKET_REGION}.amazonaws.com/${BUCKET_NAME}/data/events_{_partition_id}.json',
    'JSONEachRow',
    extra_credentials(role_arn = '${IAM_ROLE_ARN}')
);

-- Insert sample events
INSERT INTO events_s3 (event_id, user_id, event_type, event_data) VALUES
    (1, 'user123', 'page_view', '{"page": "/home"}'),
    (2, 'user456', 'click', '{"button": "signup"}'),
    (3, 'user123', 'purchase', '{"amount": 99.99}');

-- Query events
SELECT event_type, count() AS total
FROM events_s3
GROUP BY event_type;
EOF

# Example 3: Export query
cat > "example_export_query.sql" << EOF
-- Example: Export aggregated data to S3
-- First, create a source table (or use existing data)
CREATE TABLE IF NOT EXISTS user_activity
(
    user_id String,
    activity_date Date,
    actions UInt32
) ENGINE = MergeTree()
ORDER BY (user_id, activity_date);

-- Insert sample data
INSERT INTO user_activity VALUES
    ('user1', '2024-01-01', 10),
    ('user1', '2024-01-02', 15),
    ('user2', '2024-01-01', 8),
    ('user2', '2024-01-02', 12);

-- Export daily summary to S3
INSERT INTO FUNCTION s3(
    'https://s3.${BUCKET_REGION}.amazonaws.com/${BUCKET_NAME}/exports/daily_summary.parquet',
    'Parquet',
    'activity_date Date, total_users UInt64, total_actions UInt64',
    extra_credentials(role_arn = '${IAM_ROLE_ARN}')
)
SELECT
    activity_date,
    uniq(user_id) AS total_users,
    sum(actions) AS total_actions
FROM user_activity
GROUP BY activity_date
ORDER BY activity_date;

-- Verify exported data
SELECT * FROM s3(
    'https://s3.${BUCKET_REGION}.amazonaws.com/${BUCKET_NAME}/exports/daily_summary.parquet',
    'Parquet',
    extra_credentials(role_arn = '${IAM_ROLE_ARN}')
);
EOF

print_success "Generated example SQL files:"
echo "  - test_s3_queries.sql (comprehensive test suite)"
echo "  - example_logs_table.sql (log data example)"
echo "  - example_events_table.sql (event tracking example)"
echo "  - example_export_query.sql (data export example)"
echo ""

# Summary
print_success "====================================================================="
print_success "S3 Integration Test Completed Successfully!"
print_success "====================================================================="
echo ""
echo "Next steps:"
echo ""
echo "1. Connect to your ClickHouse Cloud instance:"
echo "   - Use the ClickHouse Cloud Console SQL editor, or"
echo "   - Use clickhouse-client CLI"
echo ""
echo "2. Run the test queries:"
echo "   ${GREEN}cat test_s3_queries.sql | clickhouse-client --host <your-host> --secure${NC}"
echo ""
echo "3. Check the generated files in S3:"
echo "   ${GREEN}aws s3 ls s3://${BUCKET_NAME}/test/ --recursive${NC}"
echo ""
echo "4. Try the example SQL files:"
echo "   - example_logs_table.sql"
echo "   - example_events_table.sql"
echo "   - example_export_query.sql"
echo ""
echo "Configuration Summary:"
echo "  Bucket: s3://${BUCKET_NAME}"
echo "  Region: ${BUCKET_REGION}"
echo "  IAM Role: ${IAM_ROLE_ARN}"
echo ""
print_info "Test files and examples are ready to use!"
