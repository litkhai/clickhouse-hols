version: 1.3.1
cache: true

endpoints:
  custom:
    - name: "LocalLLM"
      apiKey: "ollama"
      baseURL: "http://host.docker.internal:11434/v1/"
      models:
        default:
          - "llama3.1:8b"
        fetch: false
      titleConvo: true
      titleModel: "current_model"
      summarize: false
      summaryModel: "current_model"
      forcePrompt: false
      modelDisplayLabel: "Ollama"
      # Enable MCP tool support
      capabilities:
        tools: true
        agents: true

mcpServers:
  clickhouse:
    type: streamable-http
    url: http://mcp-clickhouse-server:3001/mcp
    timeout: 30000
