{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Iceberg with Spark and Nessie\n",
    "\n",
    "This notebook demonstrates how to use PySpark with Apache Iceberg and Nessie catalog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize Spark Session with Iceberg and Nessie\n",
    "\n",
    "The Spark configuration is automatically loaded from `spark-defaults.conf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session (configuration loaded from spark-defaults.conf)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IcebergNessieDemo\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark version:\", spark.version)\n",
    "print(\"Spark session created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Verify Catalog Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show configured catalogs\n",
    "spark.sql(\"SHOW CATALOGS\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Namespace (Database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create namespace if not exists\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS nessie.demo\")\n",
    "print(\"✓ Namespace created\")\n",
    "\n",
    "# Show namespaces\n",
    "spark.sql(\"SHOW NAMESPACES IN nessie\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Iceberg Table from Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read sample data from MinIO\n",
    "df_orders = spark.read \\\n",
    "    .format(\"parquet\") \\\n",
    "    .load(\"s3a://warehouse/data/orders.parquet\")\n",
    "\n",
    "print(f\"Loaded {df_orders.count()} orders\")\n",
    "df_orders.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Iceberg table\n",
    "df_orders.writeTo(\"nessie.demo.orders\") \\\n",
    "    .using(\"iceberg\") \\\n",
    "    .createOrReplace()\n",
    "\n",
    "print(\"✓ Iceberg table 'nessie.demo.orders' created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Query Iceberg Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the table\n",
    "spark.sql(\"SELECT * FROM nessie.demo.orders LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Perform Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orders by status\n",
    "spark.sql(\"\"\"\n",
    "    SELECT status, \n",
    "           COUNT(*) as order_count,\n",
    "           SUM(total_amount) as total_revenue,\n",
    "           AVG(total_amount) as avg_order_value\n",
    "    FROM nessie.demo.orders\n",
    "    GROUP BY status\n",
    "    ORDER BY total_revenue DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top products by revenue\n",
    "spark.sql(\"\"\"\n",
    "    SELECT product_name,\n",
    "           COUNT(*) as order_count,\n",
    "           SUM(total_amount) as total_revenue\n",
    "    FROM nessie.demo.orders\n",
    "    GROUP BY product_name\n",
    "    ORDER BY total_revenue DESC\n",
    "    LIMIT 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Customers Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read customers JSON data\n",
    "df_customers = spark.read \\\n",
    "    .format(\"json\") \\\n",
    "    .load(\"s3a://warehouse/data/customers.json\")\n",
    "\n",
    "print(f\"Loaded {df_customers.count()} customers\")\n",
    "df_customers.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Iceberg table for customers\n",
    "df_customers.writeTo(\"nessie.demo.customers\") \\\n",
    "    .using(\"iceberg\") \\\n",
    "    .createOrReplace()\n",
    "\n",
    "print(\"✓ Iceberg table 'nessie.demo.customers' created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Join Orders with Customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join analysis\n",
    "spark.sql(\"\"\"\n",
    "    SELECT c.name,\n",
    "           c.city,\n",
    "           COUNT(o.order_id) as order_count,\n",
    "           SUM(o.total_amount) as total_spent,\n",
    "           AVG(o.total_amount) as avg_order_value\n",
    "    FROM nessie.demo.orders o\n",
    "    JOIN nessie.demo.customers c ON o.customer_id = c.customer_id\n",
    "    GROUP BY c.name, c.city\n",
    "    ORDER BY total_spent DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. View Table Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show tables\n",
    "spark.sql(\"SHOW TABLES IN nessie.demo\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe table\n",
    "spark.sql(\"DESCRIBE EXTENDED nessie.demo.orders\").show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Time Travel with Iceberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show table history (snapshots)\n",
    "spark.sql(\"SELECT * FROM nessie.demo.orders.history\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show table snapshots\n",
    "spark.sql(\"SELECT * FROM nessie.demo.orders.snapshots\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Insert New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from datetime import date\n",
    "\n",
    "# Create new order\n",
    "new_orders = [\n",
    "    Row(order_id=101, customer_id=1, product_name=\"Tablet\", quantity=1, \n",
    "        unit_price=499.99, total_amount=499.99, order_date=date(2024, 1, 15), status=\"pending\"),\n",
    "    Row(order_id=102, customer_id=2, product_name=\"Smartphone\", quantity=2, \n",
    "        unit_price=899.99, total_amount=1799.98, order_date=date(2024, 1, 16), status=\"processing\")\n",
    "]\n",
    "\n",
    "new_df = spark.createDataFrame(new_orders)\n",
    "\n",
    "# Append to table\n",
    "new_df.writeTo(\"nessie.demo.orders\").append()\n",
    "\n",
    "print(\"✓ New orders inserted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify count increased\n",
    "spark.sql(\"SELECT COUNT(*) as total_orders FROM nessie.demo.orders\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Update Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update order status\n",
    "spark.sql(\"\"\"\n",
    "    UPDATE nessie.demo.orders\n",
    "    SET status = 'delivered'\n",
    "    WHERE order_id = 101\n",
    "\"\"\")\n",
    "\n",
    "print(\"✓ Order updated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify update\n",
    "spark.sql(\"SELECT * FROM nessie.demo.orders WHERE order_id = 101\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Delete Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete cancelled orders\n",
    "spark.sql(\"\"\"\n",
    "    DELETE FROM nessie.demo.orders\n",
    "    WHERE status = 'cancelled'\n",
    "\"\"\")\n",
    "\n",
    "print(\"✓ Cancelled orders deleted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Schema Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column\n",
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE nessie.demo.orders \n",
    "    ADD COLUMN discount DOUBLE\n",
    "\"\"\")\n",
    "\n",
    "print(\"✓ Column added\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show updated schema\n",
    "spark.sql(\"DESCRIBE nessie.demo.orders\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Working with Nessie Branches (Git-like versioning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynessie import init\n",
    "\n",
    "# Connect to Nessie\n",
    "nessie_client = init(\"http://nessie:19120/api/v2\")\n",
    "\n",
    "# List branches\n",
    "branches = nessie_client.list_references()\n",
    "print(\"\\nNessie branches:\")\n",
    "for branch in branches.references:\n",
    "    print(f\"  - {branch.name} ({branch.hash_[:8]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new branch for testing\n",
    "try:\n",
    "    main_branch = nessie_client.get_reference(\"main\")\n",
    "    nessie_client.create_reference(\n",
    "        \"dev\",\n",
    "        \"BRANCH\",\n",
    "        main_branch.hash_\n",
    "    )\n",
    "    print(\"✓ Created 'dev' branch\")\n",
    "except Exception as e:\n",
    "    print(f\"Branch may already exist: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "- Configuring Spark with Iceberg and Nessie\n",
    "- Creating Iceberg tables from S3/MinIO data\n",
    "- Querying and analyzing data\n",
    "- CRUD operations (Insert, Update, Delete)\n",
    "- Schema evolution\n",
    "- Time travel queries\n",
    "- Git-like branching with Nessie"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
