# ClickHouse 25.8 New Features Lab

[English](#english) | [í•œêµ­ì–´](#í•œêµ­ì–´)

---

## English

A hands-on laboratory for learning and testing ClickHouse 25.8 new features. This directory is designed for practical exercises with **integrated MinIO-based Data Lake environment**.

### ğŸ“‹ Overview

ClickHouse 25.8 includes a new Parquet Reader (1.81x faster), enhanced Data Lake integration, Hive-style partitioning, S3 temporary data storage, and improved UNION ALL functionality.

### ğŸ¯ Key Features

1. **New Parquet Reader** - 1.81x faster performance, 99.98% less data scanning
2. **MinIO Integration** - Data Lake implementation via S3-compatible storage
3. **Data Lake Enhancements** - Iceberg CREATE/DROP, Delta Lake writes, time travel
4. **Hive-Style Partitioning** - partition_strategy parameter, directory-based partitioning
5. **Temporary Data on S3** - Use S3 instead of local disk for temporary data storage
6. **Enhanced UNION ALL** - _table virtual column support

### ğŸš€ Quick Start

#### Prerequisites

- macOS (with Docker Desktop)
- [oss-mac-setup](../oss-mac-setup/) environment setup
- [datalake-minio-catalog](../datalake-minio-catalog/) auto-deployment (handled by setup script)

#### Setup and Run

```bash
# 1. Install ClickHouse 25.8 + MinIO Data Lake and start
cd local/25.8
./00-setup.sh   # Deploys ClickHouse 25.8, MinIO, and Nessie

# 2. Run tests for each feature
./01-new-parquet-reader.sh      # Local file-based Parquet Reader test
./06-minio-integration.sh       # MinIO S3 integration test (â˜… Recommended)
./02-hive-partitioning.sh
./03-temp-data-s3.sh
./04-union-all-table.sh
./05-data-lake-features.sh
```

#### What Gets Deployed

When running `./00-setup.sh`, the following are automatically deployed:

1. **MinIO** (ports 19000, 19001)
   - S3-compatible object storage
   - Web console: http://localhost:19001
   - Credentials: admin / password123

2. **Nessie** (port 19120)
   - Git-like data catalog
   - REST API: http://localhost:19120

3. **ClickHouse 25.8** (ports 2508, 25081)
   - Web UI: http://localhost:2508/play
   - TCP port: 25081

#### Manual Execution (SQL only)

To execute SQL files directly:

```bash
# Connect to ClickHouse client
cd ../oss-mac-setup
./client.sh 2508

# Execute SQL file
cd ../25.8
source 01-new-parquet-reader.sql
```

### ğŸ“š Feature Details

#### 0. MinIO Integration (06-minio-integration) â˜… Recommended

**New Feature:** Real-world Data Lake implementation with ClickHouse 25.8 + MinIO S3-compatible storage

**Test Content:**
- Generate 50,000 e-commerce order data
- Export data to MinIO in Parquet format
- Read data from MinIO using S3 functions
- Column pruning optimization (99.98% less data scanning)
- Split files by country and wildcard queries
- Daily revenue analysis (14 days)
- Product category performance analysis
- Customer segmentation analysis (VIP, Premium)

**Execute:**
```bash
./06-minio-integration.sh
```

**Key Learning Points:**
- Integration of S3-compatible storage (MinIO) with ClickHouse
- Data read/write via `s3()` function
- Real-world performance of new Parquet Reader (1.81x faster)
- Minimal data scanning through column pruning
- Multi-file queries using wildcards
- Data Lake implementation in local development environment

**Real-World Use Cases:**
- Local Data Lake development and testing
- Local validation before S3 migration
- Cost-effective data storage
- Data analytics pipeline prototyping
- E-commerce revenue analysis dashboard
- Customer behavior analysis and segmentation

**Dataset:**
- 50,000 orders (8 countries, 5,000 customers)
- 8 product categories
- 4 order statuses
- 38M+ total revenue

---

#### 1. New Parquet Reader (01-new-parquet-reader)

**New Feature:** New Parquet Reader with 1.81x faster performance and 99.98% less data scanning

**Test Content:**
- E-commerce event dataset generation (100,000 rows)
- Parquet file export
- Reading with new Parquet Reader
- Column pruning optimization (read only necessary columns)
- Complex analytical query performance
- Conversion funnel analysis
- User behavior analysis
- Device and channel performance analysis
- Geographic analysis
- Product category performance
- Time-based activity patterns

**Execute:**
```bash
./01-new-parquet-reader.sh
# Or
cat 01-new-parquet-reader.sql | docker exec -i clickhouse-25-8 clickhouse-client --multiline --multiquery
```

**Key Learning Points:**
- New Parquet Reader is 1.81x faster than previous version
- Column pruning scans 99.98% less data
- Improved memory efficiency by reading only necessary columns
- Full support for Parquet v2 format
- Reduced memory usage for large Parquet files
- Improved support for nested structures and arrays

**Real-World Use Cases:**
- Data Lake query acceleration
- Direct analysis of Parquet files on S3/GCS/Azure
- ETL pipeline optimization
- Large-scale log file analysis
- Data warehouse federated queries
- Machine learning feature engineering
- Real-time analytics dashboards
- Cost-effective cold storage queries

**Performance Comparison:**
| Task | Previous Version | ClickHouse 25.8 | Improvement |
|------|-----------------|-----------------|-------------|
| Full Parquet scan | Baseline | 1.81x faster | 81% improvement |
| Selective column read | Scans much data | 99.98% less scan | Scans only 0.02% |
| Memory usage | High | Significantly reduced | Improved efficiency |

---

#### 2. Hive-Style Partitioning (02-hive-partitioning)

**New Feature:** Hive-style directory structure support with partition_strategy parameter

**Test Content:**
- Sales transaction data generation (100,000 rows)
- Export data with Hive-style partitioning (year=YYYY/month=MM/)
- Read partitioned data
- Partition pruning (scan only specific partitions)
- Regional sales analysis
- Product category performance
- Store performance ranking
- Payment method analysis
- Daily sales trends
- High-value customer analysis
- Partition efficiency comparison
- Monthly growth rate analysis

**Execute:**
```bash
./02-hive-partitioning.sh
```

**Key Learning Points:**
- Hive partitioning: key=value directory structure
- Avoid unnecessary directory scans with partition pruning
- Standard approach compatible with Spark, Presto, Athena
- Multi-level partitioning support (year/month/day)
- Automatic partition column detection
- Add partitions without schema changes
- Data organization and query optimization

**Real-World Use Cases:**
- Data Lake organization (S3, HDFS, GCS)
- Multi-engine data sharing (Spark + ClickHouse)
- Time-series data management
- Geographic data partitioning
- Multi-tenant data isolation
- ETL pipeline optimization
- Compliance data retention
- Cost-effective data archiving

**Partition Pattern Examples:**
```
/data/year=2024/month=12/day=01/
/data/country=US/region=West/
/data/tenant_id=123/date=2024-12-01/
/data/event_type=purchase/hour=14/
```

**Performance Benefits:**
- **Reduced I/O:** Skip irrelevant partitions
- **Query speed:** Read only necessary data
- **Organization:** Intuitive directory structure
- **Scalability:** Efficiently handle petabyte-scale data

---

#### 3. Temporary Data on S3 (03-temp-data-s3)

**New Feature:** Use S3 instead of local disk for temporary data storage

**Test Content:**
- Understanding temporary data concept
- S3 temporary data configuration methods
- Large dataset generation (500,000 events)
- Large JOIN operations (using temporary storage)
- High cardinality GROUP BY aggregation
- Large-scale DISTINCT operations
- Complex window functions
- Large-scale ORDER BY sorting
- Session analysis (complex queries)
- Product performance analysis (multi-JOIN)
- User cohort analysis

**Execute:**
```bash
./03-temp-data-s3.sh
```

**Key Learning Points:**
- Temporary data is generated during query execution (JOIN, GROUP BY, ORDER BY, etc.)
- Before 25.8: Local disk only, space constraints
- After 25.8: S3 available, unlimited capacity
- Automatic spillover when memory exceeds
- Transparent operation to queries
- S3-optimized I/O patterns
- Automatic temporary data cleanup

**Operations Requiring Temporary Data:**
1. Large JOINs (hash tables)
2. High cardinality GROUP BY
3. Large data sorting (ORDER BY)
4. Window functions
5. DISTINCT operations
6. Aggregations exceeding memory limits

**Configuration Example:**
```xml
<storage_configuration>
    <disks>
        <s3_disk>
            <type>s3</type>
            <endpoint>https://bucket.s3.amazonaws.com/temp/</endpoint>
        </s3_disk>
    </disks>
</storage_configuration>
```

```sql
SET max_bytes_before_external_group_by = 10000000000;  -- 10GB
SET max_bytes_before_external_sort = 10000000000;       -- 10GB
SET temporary_data_policy = 'temp_policy';
```

**Real-World Use Cases:**
- Ad-hoc analysis of large datasets
- Complex multi-table JOINs
- High cardinality aggregations
- Data exploration and discovery
- Machine learning feature engineering
- Full dataset quality checks
- Overcome local disk constraints
- Cost-effective large-scale processing

**Benefits:**
- Can exceed local disk limits
- Better resource utilization
- Cost efficiency (S3 cheaper than local SSD)
- No manual temporary data management
- Improved query success rate

---

#### 4. Enhanced UNION ALL with _table Virtual Column (04-union-all-table)

**New Feature:** _table virtual column for source table identification in UNION ALL

**Test Content:**
- Create multi-region sales tables (US, EU, Asia, LATAM)
- Insert regional sales data
- Use UNION ALL with _table column
- Global sales analysis
- Regional daily sales trends
- Regional product performance
- Customer distribution analysis
- Filter by source table
- Cross-region performance comparison
- Weekly trend analysis
- Data lineage tracking
- Multi-currency aggregation

**Execute:**
```bash
./04-union-all-table.sh
```

**Key Learning Points:**
- _table virtual column identifies source table in UNION ALL results
- Can use _table in WHERE, GROUP BY, ORDER BY
- Enable data lineage tracking
- Source-based aggregation and filtering
- Multi-table query audit trails
- Minimal overhead

**Syntax Examples:**
```sql
SELECT *, 'table1' AS _table FROM table1
UNION ALL
SELECT *, 'table2' AS _table FROM table2

-- Filter by _table
WHERE _table = 'table1'

-- Group by _table
GROUP BY _table

-- Order by _table
ORDER BY _table, revenue DESC
```

**Real-World Use Cases:**
- Multi-region data integration
- Year-based table union (historical data)
- Multi-tenant data queries
- Federated query results
- Data migration validation
- Cross-shard analysis
- Audit and compliance
- Data governance

**Key Benefits:**
- Data lineage tracking
- Source identification in merged results
- Filter by source table
- Source-based aggregation
- Multi-table query audit trails
- Debugging and troubleshooting

---

#### 5. Data Lake Enhancements (05-data-lake-features)

**New Feature:** Iceberg CREATE/DROP, Delta Lake writes, time travel

**Test Content:**
- Data Lake overview and feature description
- Product catalog data generation (10,000 products)
- Export to Data Lake in Parquet format
- Read data from Data Lake
- Version management simulation (v1, v2, v3)
- Time travel - version comparison
- Delta Lake style incremental updates
- Iceberg style partitioning
- Schema evolution simulation
- Multi-version queries
- Data Lake metadata queries
- Point-in-time queries
- Version-based audit trails

**Execute:**
```bash
./05-data-lake-features.sh
```

**Key Learning Points:**
- Apache Iceberg table CREATE/DROP
- Delta Lake write support
- ACID transaction guarantees
- Schema evolution (without rewriting)
- Query historical versions with time travel
- Version comparison and auditing
- Metadata optimization
- Multi-format integration (Parquet, Iceberg, Delta)

**Time Travel Examples:**
```sql
-- Iceberg: Point-in-time query
SELECT * FROM iceberg_table
FOR SYSTEM_TIME AS OF '2024-12-01';

-- Delta Lake: Specific version query
SELECT * FROM delta_table
VERSION AS OF 42;

-- Timestamp-based query
SELECT * FROM delta_table
TIMESTAMP AS OF '2024-12-01 00:00:00';
```

**Real-World Use Cases:**
- Data Lake analytics
  - Direct S3/HDFS queries
  - Multi-format support
  - Partition pruning

- Data engineering:
  - ETL pipelines
  - Data quality checks
  - Schema evolution

- Data science:
  - Feature engineering
  - Historical analysis
  - Experiment tracking

- Compliance and auditing:
  - Regulatory compliance with time travel
  - Audit trails
  - Data lineage

- Real-time analytics:
  - Streaming + batch
  - Incremental updates
  - ACID guarantees

**Integration Examples:**
- **ClickHouse + Spark:** Share Iceberg tables
- **ClickHouse + Presto:** Federated queries
- **ClickHouse + Airflow:** ETL orchestration
- **ClickHouse + dbt:** Data transformation
- **ClickHouse + Kafka:** Streaming ingestion

**Performance Tips:**
- Use partitioning for large datasets
- Leverage time travel for debugging
- Implement schema evolution carefully
- Monitor metadata operations
- Optimize partition strategy
- Improve storage efficiency with compression
- Optimize file size (128MB-1GB)

### ğŸ”§ Management

#### ClickHouse Connection Info

- **Web UI**: http://localhost:2508/play
- **HTTP API**: http://localhost:2508
- **TCP**: localhost:25081
- **User**: default (no password)

#### Useful Commands

```bash
# Check ClickHouse status
cd ../oss-mac-setup
./status.sh

# Connect to CLI
./client.sh 2508

# View logs
docker logs clickhouse-25-8

# Stop
./stop.sh

# Complete removal
./stop.sh --cleanup
```

### ğŸ“‚ File Structure

```
25.8/
â”œâ”€â”€ README.md                      # This document
â”œâ”€â”€ 00-setup.sh                    # ClickHouse 25.8 installation script
â”œâ”€â”€ 01-new-parquet-reader.sh       # New Parquet Reader test execution
â”œâ”€â”€ 01-new-parquet-reader.sql      # New Parquet Reader SQL
â”œâ”€â”€ 02-hive-partitioning.sh        # Hive-style partitioning test execution
â”œâ”€â”€ 02-hive-partitioning.sql       # Hive-style partitioning SQL
â”œâ”€â”€ 03-temp-data-s3.sh             # S3 temporary data test execution
â”œâ”€â”€ 03-temp-data-s3.sql            # S3 temporary data SQL
â”œâ”€â”€ 04-union-all-table.sh          # UNION ALL test execution
â”œâ”€â”€ 04-union-all-table.sql         # UNION ALL SQL
â”œâ”€â”€ 05-data-lake-features.sh       # Data Lake features test execution
â”œâ”€â”€ 05-data-lake-features.sql      # Data Lake features SQL
â”œâ”€â”€ 06-minio-integration.sh        # MinIO integration test execution
â””â”€â”€ 06-minio-integration.sql       # MinIO integration SQL
```

### ğŸ“ Learning Path

#### For Beginners
1. **00-setup.sh** - Understand environment setup
2. **01-new-parquet-reader** - Learn Parquet file reading and performance improvements
3. **04-union-all-table** - Basics of multi-table integration

#### For Intermediate Users
1. **02-hive-partitioning** - Understand partitioning strategies
2. **03-temp-data-s3** - Learn large-scale query optimization
3. **05-data-lake-features** - Explore Data Lake integration

#### For Advanced Users
- Combine all features to implement end-to-end Data Lake pipelines
- Design real production scenarios
- Performance benchmarking and optimization
- Multi-engine integration (Spark, Presto, ClickHouse)

### ğŸ’¡ Feature Comparison

#### ClickHouse 25.8 vs Previous Versions

| Feature | Before 25.8 | ClickHouse 25.8 | Improvement |
|---------|-------------|-----------------|-------------|
| Parquet Reader | Standard | 1.81x faster | 81% faster |
| Parquet Column Pruning | Basic | 99.98% less scan | Drastically reduced I/O |
| Hive Partitioning | Manual | Native support | Standard compatibility |
| Temporary Data | Local disk only | S3 support | Unlimited capacity |
| UNION ALL | Basic | _table column | Source tracking |
| Iceberg Tables | Read-only | CREATE/DROP | Full management |
| Delta Lake | Read-only | Write support | Bidirectional |
| Time Travel | Limited | Full support | Historical queries |

#### Performance Comparison

| Operation | Previous | ClickHouse 25.8 | Benefit |
|-----------|----------|-----------------|---------|
| Parquet full scan | Baseline | 1.81x faster | Speed |
| Parquet selective columns | Many bytes | 0.02% of data | I/O efficiency |
| Hive partition query | Full scan | Partition pruning | Reduced scanning |
| Large JOIN | Memory limited | S3 spillover | Unlimited capacity |
| Multi-table union | No tracking | Source identification | Data lineage |
| Data Lake writes | Limited | Full ACID | Data integrity |

### ğŸ” Additional Resources

- **Official Release Blog**: [ClickHouse 25.8 Release](https://clickhouse.com/blog/clickhouse-release-25-08)
- **ClickHouse Documentation**: [docs.clickhouse.com](https://clickhouse.com/docs)
- **Release Notes**: [Changelog 2025](https://clickhouse.com/docs/whats-new/changelog)
- **GitHub Repository**: [ClickHouse GitHub](https://github.com/ClickHouse/ClickHouse)
- **Data Lake Formats**:
  - [Apache Iceberg](https://iceberg.apache.org/)
  - [Delta Lake](https://delta.io/)
  - [Apache Parquet](https://parquet.apache.org/)

### ğŸ“ Notes

- Each script can be executed independently
- Read and modify SQL files directly to experiment
- Test data is generated within each SQL file
- Cleanup is commented out by default
- Thorough testing recommended before production use
- Data Lake features may require appropriate storage configuration

### ğŸ”’ Security Considerations

**When accessing Data Lake:**
- Manage S3/GCS/Azure credentials securely
- Use environment variables or IAM roles
- Data encryption (in transit and at rest)
- Access control and permission management
- Enable audit logging

**When using temporary data:**
- S3 bucket access control
- Set up automatic temporary data cleanup
- Cost monitoring
- Consider network bandwidth

### âš¡ Performance Tips

**Parquet Reader optimization:**
- SELECT only necessary columns
- Filter rows with WHERE conditions
- Maintain appropriate file size (128MB-1GB)
- Choose compression algorithm (Snappy, ZSTD)

**Hive Partitioning optimization:**
- Choose partition keys matching query patterns
- Maintain appropriate partition count (not too many or too few)
- Include partition keys in WHERE conditions
- Leverage partition pruning

**Temporary data optimization:**
- Set appropriate memory thresholds
- Perform large operations during off-peak hours
- Monitor S3 usage costs
- Leverage local cache

**UNION ALL optimization:**
- Partition pruning with _table column
- Exclude unnecessary tables
- Leverage index keys

**Data Lake optimization:**
- Partition time-series data by date
- Leverage metadata caching
- Optimize file size
- Utilize predicate pushdown

### ğŸš€ Production Deployment

#### Best Practices

```sql
-- Check partition information
SELECT
    partition,
    name,
    rows,
    bytes_on_disk
FROM system.parts
WHERE table = 'your_table'
  AND active = 1;

-- Monitor running queries
SELECT
    query_id,
    user,
    query,
    elapsed,
    memory_usage
FROM system.processes
WHERE query NOT LIKE '%system.processes%';

-- Check disk usage
SELECT
    name,
    path,
    formatReadableSize(free_space) AS free,
    formatReadableSize(total_space) AS total
FROM system.disks;
```

### ğŸ›  Management Commands

#### ClickHouse Management

```bash
# Check ClickHouse status
cd ../oss-mac-setup
./status.sh

# Connect to ClickHouse CLI
./client.sh 2508

# Stop ClickHouse
./stop.sh

# Restart ClickHouse
./start.sh
```

#### Data Lake (MinIO + Nessie) Management

```bash
# Access MinIO web console
# In browser: http://localhost:19001
# Credentials: admin / password123

# Stop MinIO and Nessie
cd ../datalake-minio-catalog
docker-compose down

# Restart MinIO and Nessie
docker-compose up -d minio nessie minio-setup

# Complete MinIO data removal
docker-compose down -v

# Check container logs
docker-compose logs -f minio
docker-compose logs -f nessie
```

#### Full Environment Reconfiguration

```bash
# 1. Stop and clean all services
cd ../oss-mac-setup
./stop.sh

cd ../datalake-minio-catalog
docker-compose down -v

# 2. Full restart
cd ../25.8
./00-setup.sh
```

#### Data Verification

```bash
# Check files stored in MinIO
docker exec -it minio mc ls myminio/warehouse/

# Check ClickHouse tables
docker exec -it clickhouse-25-8 clickhouse-client -q "SHOW TABLES"

# Check ClickHouse database size
docker exec -it clickhouse-25-8 clickhouse-client -q "SELECT database, formatReadableSize(sum(bytes)) as size FROM system.parts GROUP BY database"
```

#### Migration Strategy

1. **Validate in test environment**
   - Test all new features
   - Performance benchmarking
   - Establish rollback plan

2. **Gradual rollout**
   - Start with small datasets
   - Monitor and measure performance
   - Immediate response on issues

3. **Monitoring**
   - Track query performance
   - Monitor resource usage
   - Check errors and warnings

### ğŸ¤ Contributing

If you have improvements or additional examples for this lab:
1. Register an issue
2. Submit a Pull Request
3. Share feedback

### ğŸ“„ License

MIT License - Free to learn and modify

---

**Happy Learning! ğŸš€**

For questions or issues, please refer to the main [clickhouse-hols README](../../README.md).

---

## í•œêµ­ì–´

ClickHouse 25.8 ì‹ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ë° í•™ìŠµ í™˜ê²½ì…ë‹ˆë‹¤. ì´ ë””ë ‰í† ë¦¬ëŠ” ClickHouse 25.8ì—ì„œ ìƒˆë¡­ê²Œ ì¶”ê°€ëœ ê¸°ëŠ¥ë“¤ì„ ì‹¤ìŠµí•˜ê³  ë°˜ë³µ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, **MinIO ê¸°ë°˜ Data Lake í™˜ê²½ì´ í†µí•©**ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

### ğŸ“‹ ê°œìš”

ClickHouse 25.8ì€ ìƒˆë¡œìš´ Parquet Reader (1.81ë°° ë¹ ë¥¸ ì„±ëŠ¥), Data Lake í†µí•© ê°•í™”, Hive-style íŒŒí‹°ì…”ë‹, S3 ì„ì‹œ ë°ì´í„° ì €ì¥, ê·¸ë¦¬ê³  í–¥ìƒëœ UNION ALL ê¸°ëŠ¥ì„ í¬í•¨í•©ë‹ˆë‹¤.

### ğŸ¯ ì£¼ìš” ê¸°ëŠ¥

1. **New Parquet Reader** - 1.81ë°° ë¹ ë¥¸ ì„±ëŠ¥, 99.98% ì ì€ ë°ì´í„° ìŠ¤ìº”
2. **MinIO Integration** - S3 í˜¸í™˜ ìŠ¤í† ë¦¬ì§€ë¥¼ í†µí•œ Data Lake êµ¬í˜„
3. **Data Lake Enhancements** - Iceberg CREATE/DROP, Delta Lake ì“°ê¸°, ì‹œê°„ ì—¬í–‰
4. **Hive-Style Partitioning** - partition_strategy íŒŒë¼ë¯¸í„°, ë””ë ‰í† ë¦¬ ê¸°ë°˜ íŒŒí‹°ì…”ë‹
5. **Temporary Data on S3** - ë¡œì»¬ ë””ìŠ¤í¬ ëŒ€ì‹  S3ë¥¼ ì„ì‹œ ë°ì´í„° ì €ì¥ì†Œë¡œ í™œìš©
6. **Enhanced UNION ALL** - _table ê°€ìƒ ì»¬ëŸ¼ ì§€ì›

### ğŸš€ ë¹ ë¥¸ ì‹œì‘

#### ì‚¬ì „ ìš”êµ¬ì‚¬í•­

- macOS (with Docker Desktop)
- [oss-mac-setup](../oss-mac-setup/) í™˜ê²½ êµ¬ì„±
- [datalake-minio-catalog](../datalake-minio-catalog/) ìë™ ë°°í¬ (setup ìŠ¤í¬ë¦½íŠ¸ê°€ ì²˜ë¦¬)

#### ì„¤ì • ë° ì‹¤í–‰

```bash
# 1. ClickHouse 25.8 + MinIO Data Lake ì„¤ì¹˜ ë° ì‹œì‘
cd local/25.8
./00-setup.sh   # ClickHouse 25.8, MinIO, Nessieë¥¼ ëª¨ë‘ ë°°í¬í•©ë‹ˆë‹¤

# 2. ê° ê¸°ëŠ¥ë³„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
./01-new-parquet-reader.sh      # ë¡œì»¬ íŒŒì¼ ê¸°ë°˜ Parquet Reader í…ŒìŠ¤íŠ¸
./06-minio-integration.sh       # MinIO S3 í†µí•© í…ŒìŠ¤íŠ¸ (â˜… ì¶”ì²œ)
./02-hive-partitioning.sh
./03-temp-data-s3.sh
./04-union-all-table.sh
./05-data-lake-features.sh
```

#### ë°°í¬ë˜ëŠ” í•­ëª©

`./00-setup.sh` ì‹¤í–‰ ì‹œ ë‹¤ìŒì´ ìë™ìœ¼ë¡œ ë°°í¬ë©ë‹ˆë‹¤:

1. **MinIO** (í¬íŠ¸ 19000, 19001)
   - S3 í˜¸í™˜ ê°ì²´ ìŠ¤í† ë¦¬ì§€
   - ì›¹ ì½˜ì†”: http://localhost:19001
   - ìê²©ì¦ëª…: admin / password123

2. **Nessie** (í¬íŠ¸ 19120)
   - Git-like ë°ì´í„° ì¹´íƒˆë¡œê·¸
   - REST API: http://localhost:19120

3. **ClickHouse 25.8** (í¬íŠ¸ 2508, 25081)
   - ì›¹ UI: http://localhost:2508/play
   - TCP í¬íŠ¸: 25081

#### ìˆ˜ë™ ì‹¤í–‰ (SQLë§Œ)

SQL íŒŒì¼ì„ ì§ì ‘ ì‹¤í–‰í•˜ë ¤ë©´:

```bash
# ClickHouse í´ë¼ì´ì–¸íŠ¸ ì ‘ì†
cd ../oss-mac-setup
./client.sh 2508

# SQL íŒŒì¼ ì‹¤í–‰
cd ../25.8
source 01-new-parquet-reader.sql
```

### ğŸ“š ê¸°ëŠ¥ ìƒì„¸

#### 0. MinIO Integration (06-minio-integration) â˜… ì¶”ì²œ

**ìƒˆë¡œìš´ ê¸°ëŠ¥:** ClickHouse 25.8 + MinIO S3 í˜¸í™˜ ìŠ¤í† ë¦¬ì§€ë¥¼ í†µí•œ ì‹¤ì „ Data Lake êµ¬í˜„

**í…ŒìŠ¤íŠ¸ ë‚´ìš©:**
- 50,000ê°œ ì´ì»¤ë¨¸ìŠ¤ ì£¼ë¬¸ ë°ì´í„° ìƒì„±
- MinIOë¡œ Parquet í˜•ì‹ ë°ì´í„° ë‚´ë³´ë‚´ê¸°
- S3 í•¨ìˆ˜ë¡œ MinIOì—ì„œ ë°ì´í„° ì½ê¸°
- ì»¬ëŸ¼ í”„ë£¨ë‹ ìµœì í™” (99.98% ì ì€ ë°ì´í„° ìŠ¤ìº”)
- êµ­ê°€ë³„ íŒŒì¼ ë¶„í•  ë° ì™€ì¼ë“œì¹´ë“œ ì¿¼ë¦¬
- ì¼ì¼ ë§¤ì¶œ ë¶„ì„ (14ì¼)
- ì œí’ˆ ì¹´í…Œê³ ë¦¬ ì„±ëŠ¥ ë¶„ì„
- ê³ ê° ì„¸ë¶„í™” ë¶„ì„ (VIP, Premium)

**ì‹¤í–‰:**
```bash
./06-minio-integration.sh
```

**ì£¼ìš” í•™ìŠµ í¬ì¸íŠ¸:**
- S3 í˜¸í™˜ ìŠ¤í† ë¦¬ì§€ (MinIO)ì™€ ClickHouse í†µí•©
- `s3()` í•¨ìˆ˜ë¥¼ í†µí•œ ë°ì´í„° ì½ê¸°/ì“°ê¸°
- ìƒˆë¡œìš´ Parquet Readerì˜ ì‹¤ì œ ì„±ëŠ¥ (1.81ë°° ë¹ ë¦„)
- ì»¬ëŸ¼ í”„ë£¨ë‹ì„ í†µí•œ ìµœì†Œ ë°ì´í„° ìŠ¤ìº”
- ì™€ì¼ë“œì¹´ë“œë¥¼ ì‚¬ìš©í•œ ë‹¤ì¤‘ íŒŒì¼ ì¿¼ë¦¬
- ë¡œì»¬ ê°œë°œ í™˜ê²½ì—ì„œì˜ Data Lake êµ¬í˜„

**ì‹¤ë¬´ í™œìš©:**
- ë¡œì»¬ Data Lake ê°œë°œ ë° í…ŒìŠ¤íŠ¸
- S3 ë§ˆì´ê·¸ë ˆì´ì…˜ ì „ ë¡œì»¬ ê²€ì¦
- ë¹„ìš© íš¨ìœ¨ì ì¸ ë°ì´í„° ì €ì¥ì†Œ
- ë°ì´í„° ë¶„ì„ íŒŒì´í”„ë¼ì¸ í”„ë¡œí† íƒ€ì…
- ì´ì»¤ë¨¸ìŠ¤ ë§¤ì¶œ ë¶„ì„ ëŒ€ì‹œë³´ë“œ
- ê³ ê° í–‰ë™ ë¶„ì„ ë° ì„¸ë¶„í™”

**ë°ì´í„°ì…‹:**
- 50,000ê°œ ì£¼ë¬¸ (8ê°œ êµ­ê°€, 5,000ëª… ê³ ê°)
- 8ê°œ ì œí’ˆ ì¹´í…Œê³ ë¦¬
- 4ê°€ì§€ ì£¼ë¬¸ ìƒíƒœ
- 38M+ ì´ ë§¤ì¶œ

---

#### 1. New Parquet Reader (01-new-parquet-reader)

**ìƒˆë¡œìš´ ê¸°ëŠ¥:** 1.81ë°° ë¹ ë¥¸ ì„±ëŠ¥ê³¼ 99.98% ì ì€ ë°ì´í„° ìŠ¤ìº”ì„ ì œê³µí•˜ëŠ” ìƒˆë¡œìš´ Parquet Reader

**í…ŒìŠ¤íŠ¸ ë‚´ìš©:**
- E-commerce ì´ë²¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„± (100,000 í–‰)
- Parquet íŒŒì¼ ë‚´ë³´ë‚´ê¸°
- ìƒˆë¡œìš´ Parquet Readerë¡œ ì½ê¸°
- Column pruning ìµœì í™” (í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì½ê¸°)
- ë³µì¡í•œ ë¶„ì„ ì¿¼ë¦¬ ì„±ëŠ¥
- ë³€í™˜ìœ¨ í¼ë„ ë¶„ì„
- ì‚¬ìš©ì í–‰ë™ ë¶„ì„
- ë””ë°”ì´ìŠ¤ ë° ì±„ë„ ì„±ëŠ¥ ë¶„ì„
- ì§€ë¦¬ì  ë¶„ì„
- ì œí’ˆ ì¹´í…Œê³ ë¦¬ ì„±ëŠ¥
- ì‹œê°„ëŒ€ë³„ í™œë™ íŒ¨í„´

**ì‹¤í–‰:**
```bash
./01-new-parquet-reader.sh
# ë˜ëŠ”
cat 01-new-parquet-reader.sql | docker exec -i clickhouse-25-8 clickhouse-client --multiline --multiquery
```

**ì£¼ìš” í•™ìŠµ í¬ì¸íŠ¸:**
- ìƒˆë¡œìš´ Parquet ReaderëŠ” ê¸°ì¡´ ëŒ€ë¹„ 1.81ë°° ë¹ ë¥¸ ì„±ëŠ¥
- Column pruningìœ¼ë¡œ 99.98% ì ì€ ë°ì´í„° ìŠ¤ìº”
- í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì½ì–´ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í–¥ìƒ
- Parquet v2 í¬ë§· ì™„ì „ ì§€ì›
- ëŒ€ê·œëª¨ Parquet íŒŒì¼ì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ
- ì¤‘ì²©ëœ êµ¬ì¡°ì²´ ë° ë°°ì—´ ì§€ì› ê°œì„ 

**ì‹¤ë¬´ í™œìš©:**
- Data Lake ì¿¼ë¦¬ ê°€ì†í™”
- S3/GCS/Azureì˜ Parquet íŒŒì¼ ì§ì ‘ ë¶„ì„
- ETL íŒŒì´í”„ë¼ì¸ ìµœì í™”
- ëŒ€ìš©ëŸ‰ ë¡œê·¸ íŒŒì¼ ë¶„ì„
- ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ ì—°í•© ì¿¼ë¦¬
- ê¸°ê³„ í•™ìŠµ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§
- ì‹¤ì‹œê°„ ë¶„ì„ ëŒ€ì‹œë³´ë“œ
- ë¹„ìš© íš¨ìœ¨ì ì¸ cold storage ì¿¼ë¦¬

**ì„±ëŠ¥ ë¹„êµ:**
| ì‘ì—… | ì´ì „ ë²„ì „ | ClickHouse 25.8 | ê°œì„  |
|------|----------|-----------------|------|
| ì „ì²´ Parquet ìŠ¤ìº” | ê¸°ì¤€ | 1.81ë°° ë¹ ë¦„ | 81% í–¥ìƒ |
| ì„ íƒì  ì»¬ëŸ¼ ì½ê¸° | ë§ì€ ë°ì´í„° ìŠ¤ìº” | 99.98% ì ì€ ìŠ¤ìº” | ê±°ì˜ 0.02%ë§Œ ìŠ¤ìº” |
| ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ | ë†’ìŒ | ëŒ€í­ ê°ì†Œ | íš¨ìœ¨ì„± í–¥ìƒ |

---

#### 2. Hive-Style Partitioning (02-hive-partitioning)

**ìƒˆë¡œìš´ ê¸°ëŠ¥:** partition_strategy íŒŒë¼ë¯¸í„°ë¡œ Hive-style ë””ë ‰í† ë¦¬ êµ¬ì¡° ì§€ì›

**í…ŒìŠ¤íŠ¸ ë‚´ìš©:**
- íŒë§¤ íŠ¸ëœì­ì…˜ ë°ì´í„° ìƒì„± (100,000 í–‰)
- Hive-style íŒŒí‹°ì…”ë‹ìœ¼ë¡œ ë°ì´í„° ë‚´ë³´ë‚´ê¸° (year=YYYY/month=MM/)
- íŒŒí‹°ì…˜ëœ ë°ì´í„° ì½ê¸°
- íŒŒí‹°ì…˜ í”„ë£¨ë‹ (íŠ¹ì • íŒŒí‹°ì…˜ë§Œ ìŠ¤ìº”)
- ì§€ì—­ë³„ íŒë§¤ ë¶„ì„
- ì œí’ˆ ì¹´í…Œê³ ë¦¬ ì„±ëŠ¥
- ë§¤ì¥ ì„±ëŠ¥ ìˆœìœ„
- ê²°ì œ ë°©ë²• ë¶„ì„
- ì¼ë³„ íŒë§¤ íŠ¸ë Œë“œ
- ê³ ê°€ì¹˜ ê³ ê° ë¶„ì„
- íŒŒí‹°ì…˜ íš¨ìœ¨ì„± ë¹„êµ
- ì›”ë³„ ì„±ì¥ë¥  ë¶„ì„

**ì‹¤í–‰:**
```bash
./02-hive-partitioning.sh
```

**ì£¼ìš” í•™ìŠµ í¬ì¸íŠ¸:**
- Hive partitioning: key=value ë””ë ‰í† ë¦¬ êµ¬ì¡°
- íŒŒí‹°ì…˜ í”„ë£¨ë‹ìœ¼ë¡œ ë¶ˆí•„ìš”í•œ ë””ë ‰í† ë¦¬ ìŠ¤ìº” íšŒí”¼
- Spark, Presto, Athenaì™€ í˜¸í™˜ë˜ëŠ” í‘œì¤€ ë°©ì‹
- ë‹¤ë‹¨ê³„ íŒŒí‹°ì…”ë‹ ì§€ì› (year/month/day)
- ìë™ íŒŒí‹°ì…˜ ì»¬ëŸ¼ ê°ì§€
- ìŠ¤í‚¤ë§ˆ ë³€ê²½ ì—†ì´ íŒŒí‹°ì…˜ ì¶”ê°€ ê°€ëŠ¥
- ë°ì´í„° ì¡°ì§í™” ë° ì¿¼ë¦¬ ìµœì í™”

**ì‹¤ë¬´ í™œìš©:**
- Data Lake ì¡°ì§í™” (S3, HDFS, GCS)
- ë‹¤ì¤‘ ì—”ì§„ ë°ì´í„° ê³µìœ  (Spark + ClickHouse)
- ì‹œê³„ì—´ ë°ì´í„° ê´€ë¦¬
- ì§€ë¦¬ì  ë°ì´í„° íŒŒí‹°ì…”ë‹
- ë©€í‹° í…Œë„ŒíŠ¸ ë°ì´í„° ê²©ë¦¬
- ETL íŒŒì´í”„ë¼ì¸ ìµœì í™”
- ê·œì • ì¤€ìˆ˜ ë°ì´í„° ë³´ê´€
- ë¹„ìš© íš¨ìœ¨ì ì¸ ë°ì´í„° ì•„ì¹´ì´ë¹™

**íŒŒí‹°ì…˜ íŒ¨í„´ ì˜ˆì‹œ:**
```
/data/year=2024/month=12/day=01/
/data/country=US/region=West/
/data/tenant_id=123/date=2024-12-01/
/data/event_type=purchase/hour=14/
```

**ì„±ëŠ¥ ì´ì :**
- **I/O ê°ì†Œ:** ê´€ë ¨ ì—†ëŠ” íŒŒí‹°ì…˜ ê±´ë„ˆë›°ê¸°
- **ì¿¼ë¦¬ ì†ë„:** í•„ìš”í•œ ë°ì´í„°ë§Œ ì½ê¸°
- **ì¡°ì§í™”:** ì§ê´€ì ì¸ ë””ë ‰í† ë¦¬ êµ¬ì¡°
- **í™•ì¥ì„±:** í˜íƒ€ë°”ì´íŠ¸ê¸‰ ë°ì´í„° íš¨ìœ¨ì  ì²˜ë¦¬

---

#### 3. Temporary Data on S3 (03-temp-data-s3)

**ìƒˆë¡œìš´ ê¸°ëŠ¥:** ë¡œì»¬ ë””ìŠ¤í¬ ëŒ€ì‹  S3ë¥¼ ì„ì‹œ ë°ì´í„° ì €ì¥ì†Œë¡œ ì‚¬ìš© ê°€ëŠ¥

**í…ŒìŠ¤íŠ¸ ë‚´ìš©:**
- ì„ì‹œ ë°ì´í„° ê°œë… ì´í•´
- S3 ì„ì‹œ ë°ì´í„° êµ¬ì„± ë°©ë²•
- ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ ìƒì„± (500,000 ì´ë²¤íŠ¸)
- ëŒ€ê·œëª¨ JOIN ì—°ì‚° (ì„ì‹œ ì €ì¥ì†Œ ì‚¬ìš©)
- ë†’ì€ ì¹´ë””ë„ë¦¬í‹° GROUP BY ì§‘ê³„
- ëŒ€ê·œëª¨ DISTINCT ì—°ì‚°
- ë³µì¡í•œ ìœˆë„ìš° í•¨ìˆ˜
- ëŒ€ê·œëª¨ ORDER BY ì •ë ¬
- ì„¸ì…˜ ë¶„ì„ (ë³µì¡í•œ ì¿¼ë¦¬)
- ì œí’ˆ ì„±ëŠ¥ ë¶„ì„ (ë‹¤ì¤‘ JOIN)
- ì‚¬ìš©ì ì½”í˜¸íŠ¸ ë¶„ì„

**ì‹¤í–‰:**
```bash
./03-temp-data-s3.sh
```

**ì£¼ìš” í•™ìŠµ í¬ì¸íŠ¸:**
- ì„ì‹œ ë°ì´í„°ëŠ” ì¿¼ë¦¬ ì‹¤í–‰ ì¤‘ ìƒì„±ë¨ (JOIN, GROUP BY, ORDER BY ë“±)
- 25.8 ì´ì „: ë¡œì»¬ ë””ìŠ¤í¬ë§Œ ì‚¬ìš©, ê³µê°„ ì œì•½
- 25.8 ì´í›„: S3 ì‚¬ìš© ê°€ëŠ¥, ë¬´ì œí•œ ìš©ëŸ‰
- ë©”ëª¨ë¦¬ ì´ˆê³¼ ì‹œ ìë™ spillover
- ì¿¼ë¦¬ì— íˆ¬ëª…í•˜ê²Œ ë™ì‘
- S3ì— ìµœì í™”ëœ I/O íŒ¨í„´
- ì„ì‹œ ë°ì´í„° ìë™ ì •ë¦¬

**ì„ì‹œ ë°ì´í„°ê°€ í•„ìš”í•œ ì‘ì—…:**
1. ëŒ€ê·œëª¨ JOIN (í•´ì‹œ í…Œì´ë¸”)
2. ë†’ì€ ì¹´ë””ë„ë¦¬í‹° GROUP BY
3. ëŒ€ìš©ëŸ‰ ë°ì´í„° ì •ë ¬ (ORDER BY)
4. ìœˆë„ìš° í•¨ìˆ˜
5. DISTINCT ì—°ì‚°
6. ë©”ëª¨ë¦¬ ì œí•œ ì´ˆê³¼ ì§‘ê³„

**ì„¤ì • ì˜ˆì‹œ:**
```xml
<storage_configuration>
    <disks>
        <s3_disk>
            <type>s3</type>
            <endpoint>https://bucket.s3.amazonaws.com/temp/</endpoint>
        </s3_disk>
    </disks>
</storage_configuration>
```

```sql
SET max_bytes_before_external_group_by = 10000000000;  -- 10GB
SET max_bytes_before_external_sort = 10000000000;       -- 10GB
SET temporary_data_policy = 'temp_policy';
```

**ì‹¤ë¬´ í™œìš©:**
- ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ ì• ë“œí˜¹ ë¶„ì„
- ë³µì¡í•œ ë‹¤ì¤‘ í…Œì´ë¸” JOIN
- ë†’ì€ ì¹´ë””ë„ë¦¬í‹° ì§‘ê³„
- ë°ì´í„° íƒìƒ‰ ë° ë°œê²¬
- ê¸°ê³„ í•™ìŠµ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§
- ì „ì²´ ë°ì´í„°ì…‹ í’ˆì§ˆ ê²€ì‚¬
- ë¡œì»¬ ë””ìŠ¤í¬ ì œì•½ ê·¹ë³µ
- ë¹„ìš© íš¨ìœ¨ì ì¸ ëŒ€ìš©ëŸ‰ ì²˜ë¦¬

**ì´ì :**
- ë¡œì»¬ ë””ìŠ¤í¬ í•œê³„ ì´ˆê³¼ ê°€ëŠ¥
- ë” ë‚˜ì€ ë¦¬ì†ŒìŠ¤ í™œìš©
- ë¹„ìš© íš¨ìœ¨ì„± (S3ê°€ ë¡œì»¬ SSDë³´ë‹¤ ì €ë ´)
- ìˆ˜ë™ ì„ì‹œ ë°ì´í„° ê´€ë¦¬ ë¶ˆí•„ìš”
- ì¿¼ë¦¬ ì„±ê³µë¥  í–¥ìƒ

---

#### 4. Enhanced UNION ALL with _table Virtual Column (04-union-all-table)

**ìƒˆë¡œìš´ ê¸°ëŠ¥:** UNION ALLì—ì„œ _table ê°€ìƒ ì»¬ëŸ¼ìœ¼ë¡œ ì†ŒìŠ¤ í…Œì´ë¸” ì‹ë³„ ê°€ëŠ¥

**í…ŒìŠ¤íŠ¸ ë‚´ìš©:**
- ë‹¤ì¤‘ ì§€ì—­ íŒë§¤ í…Œì´ë¸” ìƒì„± (US, EU, Asia, LATAM)
- ì§€ì—­ë³„ íŒë§¤ ë°ì´í„° ì‚½ì…
- UNION ALLê³¼ _table ì»¬ëŸ¼ ì‚¬ìš©
- ê¸€ë¡œë²Œ íŒë§¤ ë¶„ì„
- ì§€ì—­ë³„ ì¼ë³„ íŒë§¤ íŠ¸ë Œë“œ
- ì§€ì—­ë³„ ì œí’ˆ ì„±ëŠ¥
- ê³ ê° ë¶„í¬ ë¶„ì„
- ì†ŒìŠ¤ í…Œì´ë¸”ë³„ í•„í„°ë§
- ì§€ì—­ ê°„ ì„±ëŠ¥ ë¹„êµ
- ì£¼ê°„ íŠ¸ë Œë“œ ë¶„ì„
- ë°ì´í„° ê³„ë³´ ì¶”ì 
- ë‹¤ì¤‘ í†µí™” ì§‘ê³„

**ì‹¤í–‰:**
```bash
./04-union-all-table.sh
```

**ì£¼ìš” í•™ìŠµ í¬ì¸íŠ¸:**
- _table ê°€ìƒ ì»¬ëŸ¼ì´ UNION ALL ê²°ê³¼ì—ì„œ ì†ŒìŠ¤ í…Œì´ë¸” ì‹ë³„
- WHERE, GROUP BY, ORDER BYì—ì„œ _table ì‚¬ìš© ê°€ëŠ¥
- ë°ì´í„° ê³„ë³´ ì¶”ì  ê°€ëŠ¥
- ì†ŒìŠ¤ë³„ ì§‘ê³„ ë° í•„í„°ë§
- ë‹¤ì¤‘ í…Œì´ë¸” ì¿¼ë¦¬ ê°ì‚¬ ì¶”ì 
- ìµœì†Œí•œì˜ ì˜¤ë²„í—¤ë“œ

**êµ¬ë¬¸ ì˜ˆì‹œ:**
```sql
SELECT *, 'table1' AS _table FROM table1
UNION ALL
SELECT *, 'table2' AS _table FROM table2

-- _tableë¡œ í•„í„°ë§
WHERE _table = 'table1'

-- _tableë¡œ ê·¸ë£¹í™”
GROUP BY _table

-- _tableë¡œ ì •ë ¬
ORDER BY _table, revenue DESC
```

**ì‹¤ë¬´ í™œìš©:**
- ë‹¤ì¤‘ ì§€ì—­ ë°ì´í„° í†µí•©
- ì—°ë„ë³„ í…Œì´ë¸” union (íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„°)
- ë©€í‹° í…Œë„ŒíŠ¸ ë°ì´í„° ì¿¼ë¦¬
- ì—°í•© ì¿¼ë¦¬ ê²°ê³¼
- ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ê²€ì¦
- ìƒ¤ë“œ ê°„ ë¶„ì„
- ê°ì‚¬ ë° ê·œì • ì¤€ìˆ˜
- ë°ì´í„° ê±°ë²„ë„ŒìŠ¤

**ì£¼ìš” ì´ì :**
- ë°ì´í„° ê³„ë³´ ì¶”ì 
- ë³‘í•©ëœ ê²°ê³¼ì—ì„œ ì†ŒìŠ¤ ì‹ë³„
- ì†ŒìŠ¤ í…Œì´ë¸”ë³„ í•„í„°ë§
- ì†ŒìŠ¤ë³„ ì§‘ê³„
- ë‹¤ì¤‘ í…Œì´ë¸” ì¿¼ë¦¬ ê°ì‚¬ ì¶”ì 
- ë””ë²„ê¹… ë° ë¬¸ì œ í•´ê²°

---

#### 5. Data Lake Enhancements (05-data-lake-features)

**ìƒˆë¡œìš´ ê¸°ëŠ¥:** Iceberg CREATE/DROP, Delta Lake ì“°ê¸°, ì‹œê°„ ì—¬í–‰ (time travel)

**í…ŒìŠ¤íŠ¸ ë‚´ìš©:**
- Data Lake ê°œìš” ë° ê¸°ëŠ¥ ì„¤ëª…
- ì œí’ˆ ì¹´íƒˆë¡œê·¸ ë°ì´í„° ìƒì„± (10,000 ì œí’ˆ)
- Parquet í˜•ì‹ìœ¼ë¡œ Data Lake ë‚´ë³´ë‚´ê¸°
- Data Lakeì—ì„œ ë°ì´í„° ì½ê¸°
- ë²„ì „ ê´€ë¦¬ ì‹œë®¬ë ˆì´ì…˜ (v1, v2, v3)
- ì‹œê°„ ì—¬í–‰ - ë²„ì „ ë¹„êµ
- Delta Lake ìŠ¤íƒ€ì¼ ì¦ë¶„ ì—…ë°ì´íŠ¸
- Iceberg ìŠ¤íƒ€ì¼ íŒŒí‹°ì…”ë‹
- ìŠ¤í‚¤ë§ˆ ì§„í™” ì‹œë®¬ë ˆì´ì…˜
- ë‹¤ì¤‘ ë²„ì „ ì¿¼ë¦¬
- Data Lake ë©”íƒ€ë°ì´í„° ì¿¼ë¦¬
- íŠ¹ì • ì‹œì  ì¿¼ë¦¬ (Point-in-Time)
- ë²„ì „ë³„ ê°ì‚¬ ì¶”ì 

**ì‹¤í–‰:**
```bash
./05-data-lake-features.sh
```

**ì£¼ìš” í•™ìŠµ í¬ì¸íŠ¸:**
- Apache Iceberg í…Œì´ë¸” ìƒì„±/ì‚­ì œ
- Delta Lake ì“°ê¸° ì§€ì›
- ACID íŠ¸ëœì­ì…˜ ë³´ì¥
- ìŠ¤í‚¤ë§ˆ ì§„í™” (ì¬ì‘ì„± ì—†ì´)
- ì‹œê°„ ì—¬í–‰ë¡œ íˆìŠ¤í† ë¦¬ì»¬ ë²„ì „ ì¿¼ë¦¬
- ë²„ì „ ë¹„êµ ë° ê°ì‚¬
- ë©”íƒ€ë°ì´í„° ìµœì í™”
- ë‹¤ì¤‘ í¬ë§· í†µí•© (Parquet, Iceberg, Delta)

**ì‹œê°„ ì—¬í–‰ ì˜ˆì‹œ:**
```sql
-- Iceberg: íŠ¹ì • ì‹œì  ì¿¼ë¦¬
SELECT * FROM iceberg_table
FOR SYSTEM_TIME AS OF '2024-12-01';

-- Delta Lake: íŠ¹ì • ë²„ì „ ì¿¼ë¦¬
SELECT * FROM delta_table
VERSION AS OF 42;

-- íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜ ì¿¼ë¦¬
SELECT * FROM delta_table
TIMESTAMP AS OF '2024-12-01 00:00:00';
```

**ì‹¤ë¬´ í™œìš©:**
- Data Lake ë¶„ì„
  - S3/HDFS ì§ì ‘ ì¿¼ë¦¬
  - ë‹¤ì¤‘ í¬ë§· ì§€ì›
  - íŒŒí‹°ì…˜ í”„ë£¨ë‹

- ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§:
  - ETL íŒŒì´í”„ë¼ì¸
  - ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬
  - ìŠ¤í‚¤ë§ˆ ì§„í™”

- ë°ì´í„° ê³¼í•™:
  - íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§
  - íˆìŠ¤í† ë¦¬ì»¬ ë¶„ì„
  - ì‹¤í—˜ ì¶”ì 

- ê·œì • ì¤€ìˆ˜ ë° ê°ì‚¬:
  - ì‹œê°„ ì—¬í–‰ë¡œ ê·œì • ì¤€ìˆ˜
  - ê°ì‚¬ ì¶”ì 
  - ë°ì´í„° ê³„ë³´

- ì‹¤ì‹œê°„ ë¶„ì„:
  - ìŠ¤íŠ¸ë¦¬ë° + ë°°ì¹˜
  - ì¦ë¶„ ì—…ë°ì´íŠ¸
  - ACID ë³´ì¥

**í†µí•© ì˜ˆì‹œ:**
- **ClickHouse + Spark:** Iceberg í…Œì´ë¸” ê³µìœ 
- **ClickHouse + Presto:** ì—°í•© ì¿¼ë¦¬
- **ClickHouse + Airflow:** ETL ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜
- **ClickHouse + dbt:** ë°ì´í„° ë³€í™˜
- **ClickHouse + Kafka:** ìŠ¤íŠ¸ë¦¬ë° ìˆ˜ì§‘

**ì„±ëŠ¥ íŒ:**
- ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì— íŒŒí‹°ì…”ë‹ ì‚¬ìš©
- ì‹œê°„ ì—¬í–‰ì„ ë””ë²„ê¹…ì— í™œìš©
- ìŠ¤í‚¤ë§ˆ ì§„í™”ë¥¼ ì‹ ì¤‘í•˜ê²Œ êµ¬í˜„
- ë©”íƒ€ë°ì´í„° ì‘ì—… ëª¨ë‹ˆí„°ë§
- íŒŒí‹°ì…˜ ì „ëµ ìµœì í™”
- ì••ì¶•ìœ¼ë¡œ ì €ì¥ íš¨ìœ¨ì„± í–¥ìƒ
- íŒŒì¼ í¬ê¸° ìµœì í™” (128MB-1GB)

### ğŸ”§ ê´€ë¦¬

#### ClickHouse ì ‘ì† ì •ë³´

- **Web UI**: http://localhost:2508/play
- **HTTP API**: http://localhost:2508
- **TCP**: localhost:25081
- **User**: default (no password)

#### ìœ ìš©í•œ ëª…ë ¹ì–´

```bash
# ClickHouse ìƒíƒœ í™•ì¸
cd ../oss-mac-setup
./status.sh

# CLI ì ‘ì†
./client.sh 2508

# ë¡œê·¸ í™•ì¸
docker logs clickhouse-25-8

# ì¤‘ì§€
./stop.sh

# ì™„ì „ ì‚­ì œ
./stop.sh --cleanup
```

### ğŸ“‚ íŒŒì¼ êµ¬ì¡°

```
25.8/
â”œâ”€â”€ README.md                      # ì´ ë¬¸ì„œ
â”œâ”€â”€ 00-setup.sh                    # ClickHouse 25.8 ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ 01-new-parquet-reader.sh       # New Parquet Reader í…ŒìŠ¤íŠ¸ ì‹¤í–‰
â”œâ”€â”€ 01-new-parquet-reader.sql      # New Parquet Reader SQL
â”œâ”€â”€ 02-hive-partitioning.sh        # Hive-style íŒŒí‹°ì…”ë‹ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
â”œâ”€â”€ 02-hive-partitioning.sql       # Hive-style íŒŒí‹°ì…”ë‹ SQL
â”œâ”€â”€ 03-temp-data-s3.sh             # S3 ì„ì‹œ ë°ì´í„° í…ŒìŠ¤íŠ¸ ì‹¤í–‰
â”œâ”€â”€ 03-temp-data-s3.sql            # S3 ì„ì‹œ ë°ì´í„° SQL
â”œâ”€â”€ 04-union-all-table.sh          # UNION ALL í…ŒìŠ¤íŠ¸ ì‹¤í–‰
â”œâ”€â”€ 04-union-all-table.sql         # UNION ALL SQL
â”œâ”€â”€ 05-data-lake-features.sh       # Data Lake ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
â”œâ”€â”€ 05-data-lake-features.sql      # Data Lake ê¸°ëŠ¥ SQL
â”œâ”€â”€ 06-minio-integration.sh        # MinIO í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰
â””â”€â”€ 06-minio-integration.sql       # MinIO í†µí•© SQL
```

### ğŸ“ í•™ìŠµ ê²½ë¡œ

#### ì´ˆê¸‰ ì‚¬ìš©ì
1. **00-setup.sh** - í™˜ê²½ êµ¬ì„± ì´í•´
2. **01-new-parquet-reader** - Parquet íŒŒì¼ ì½ê¸°ì™€ ì„±ëŠ¥ ê°œì„  í•™ìŠµ
3. **04-union-all-table** - ë‹¤ì¤‘ í…Œì´ë¸” í†µí•© ê¸°ì´ˆ

#### ì¤‘ê¸‰ ì‚¬ìš©ì
1. **02-hive-partitioning** - íŒŒí‹°ì…”ë‹ ì „ëµ ì´í•´
2. **03-temp-data-s3** - ëŒ€ìš©ëŸ‰ ì¿¼ë¦¬ ìµœì í™” í•™ìŠµ
3. **05-data-lake-features** - Data Lake í†µí•© íƒìƒ‰

#### ê³ ê¸‰ ì‚¬ìš©ì
- ëª¨ë“  ê¸°ëŠ¥ì„ ì¡°í•©í•˜ì—¬ ì—”ë“œíˆ¬ì—”ë“œ Data Lake íŒŒì´í”„ë¼ì¸ êµ¬í˜„
- ì‹¤ì œ í”„ë¡œë•ì…˜ ì‹œë‚˜ë¦¬ì˜¤ ì„¤ê³„
- ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ ë° ìµœì í™”
- ë‹¤ì¤‘ ì—”ì§„ í†µí•© (Spark, Presto, ClickHouse)

### ğŸ’¡ ê¸°ëŠ¥ ë¹„êµ

#### ClickHouse 25.8 vs Previous Versions

| Feature | Before 25.8 | ClickHouse 25.8 | Improvement |
|---------|-------------|-----------------|-------------|
| Parquet Reader | Standard | 1.81x faster | 81% faster |
| Parquet Column Pruning | Basic | 99.98% less scan | Drastically reduced I/O |
| Hive Partitioning | Manual | Native support | Standard compatibility |
| Temporary Data | Local disk only | S3 support | Unlimited capacity |
| UNION ALL | Basic | _table column | Source tracking |
| Iceberg Tables | Read-only | CREATE/DROP | Full management |
| Delta Lake | Read-only | Write support | Bidirectional |
| Time Travel | Limited | Full support | Historical queries |

#### Performance Comparison

| Operation | Previous | ClickHouse 25.8 | Benefit |
|-----------|----------|-----------------|---------|
| Parquet full scan | Baseline | 1.81x faster | Speed |
| Parquet selective columns | Many bytes | 0.02% of data | I/O efficiency |
| Hive partition query | Full scan | Partition pruning | Reduced scanning |
| Large JOIN | Memory limited | S3 spillover | Unlimited capacity |
| Multi-table union | No tracking | Source identification | Data lineage |
| Data Lake writes | Limited | Full ACID | Data integrity |

### ğŸ” ì¶”ê°€ ìë£Œ

- **Official Release Blog**: [ClickHouse 25.8 Release](https://clickhouse.com/blog/clickhouse-release-25-08)
- **ClickHouse Documentation**: [docs.clickhouse.com](https://clickhouse.com/docs)
- **Release Notes**: [Changelog 2025](https://clickhouse.com/docs/whats-new/changelog)
- **GitHub Repository**: [ClickHouse GitHub](https://github.com/ClickHouse/ClickHouse)
- **Data Lake Formats**:
  - [Apache Iceberg](https://iceberg.apache.org/)
  - [Delta Lake](https://delta.io/)
  - [Apache Parquet](https://parquet.apache.org/)

### ğŸ“ ì°¸ê³ ì‚¬í•­

- ê° ìŠ¤í¬ë¦½íŠ¸ëŠ” ë…ë¦½ì ìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•©ë‹ˆë‹¤
- SQL íŒŒì¼ì„ ì§ì ‘ ì½ê³  ìˆ˜ì •í•˜ì—¬ ì‹¤í—˜í•´ë³´ì„¸ìš”
- í…ŒìŠ¤íŠ¸ ë°ì´í„°ëŠ” ê° SQL íŒŒì¼ ë‚´ì—ì„œ ìƒì„±ë©ë‹ˆë‹¤
- ì •ë¦¬(cleanup)ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ì£¼ì„ ì²˜ë¦¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤
- í”„ë¡œë•ì…˜ í™˜ê²½ ì ìš© ì „ ì¶©ë¶„í•œ í…ŒìŠ¤íŠ¸ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤
- Data Lake ê¸°ëŠ¥ì€ ì ì ˆí•œ ìŠ¤í† ë¦¬ì§€ êµ¬ì„±ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤

### ğŸ”’ ë³´ì•ˆ ê³ ë ¤ì‚¬í•­

**Data Lake ì ‘ê·¼ ì‹œ:**
- S3/GCS/Azure ìê²© ì¦ëª…ì„ ì•ˆì „í•˜ê²Œ ê´€ë¦¬
- í™˜ê²½ ë³€ìˆ˜ ë˜ëŠ” IAM ì—­í•  ì‚¬ìš©
- ë°ì´í„° ì•”í˜¸í™” (ì „ì†¡ ì¤‘ ë° ì €ì¥ ì‹œ)
- ì ‘ê·¼ ì œì–´ ë° ê¶Œí•œ ê´€ë¦¬
- ê°ì‚¬ ë¡œê¹… í™œì„±í™”

**ì„ì‹œ ë°ì´í„° ì‚¬ìš© ì‹œ:**
- S3 ë²„í‚· ì•¡ì„¸ìŠ¤ ì œì–´
- ì„ì‹œ ë°ì´í„° ìë™ ì •ë¦¬ ì„¤ì •
- ë¹„ìš© ëª¨ë‹ˆí„°ë§
- ë„¤íŠ¸ì›Œí¬ ëŒ€ì—­í­ ê³ ë ¤

### âš¡ ì„±ëŠ¥ íŒ

**Parquet Reader ìµœì í™”:**
- í•„ìš”í•œ ì»¬ëŸ¼ë§Œ SELECT
- WHERE ì¡°ê±´ìœ¼ë¡œ í–‰ í•„í„°ë§
- ì ì ˆí•œ íŒŒì¼ í¬ê¸° ìœ ì§€ (128MB-1GB)
- ì••ì¶• ì•Œê³ ë¦¬ì¦˜ ì„ íƒ (Snappy, ZSTD)

**Hive Partitioning ìµœì í™”:**
- ì¿¼ë¦¬ íŒ¨í„´ì— ë§ëŠ” íŒŒí‹°ì…˜ í‚¤ ì„ íƒ
- íŒŒí‹°ì…˜ ìˆ˜ë¥¼ ì ì ˆíˆ ìœ ì§€ (ë„ˆë¬´ ë§ê±°ë‚˜ ì ì§€ ì•Šê²Œ)
- íŒŒí‹°ì…˜ í‚¤ë¥¼ WHERE ì¡°ê±´ì— í¬í•¨
- íŒŒí‹°ì…˜ í”„ë£¨ë‹ í™œìš©

**ì„ì‹œ ë°ì´í„° ìµœì í™”:**
- ì ì ˆí•œ ë©”ëª¨ë¦¬ ì„ê³„ê°’ ì„¤ì •
- ì˜¤í”„í”¼í¬ ì‹œê°„ì— ëŒ€ìš©ëŸ‰ ì‘ì—… ìˆ˜í–‰
- S3 ì‚¬ìš© ë¹„ìš© ëª¨ë‹ˆí„°ë§
- ë¡œì»¬ ìºì‹œ í™œìš©

**UNION ALL ìµœì í™”:**
- _table ì»¬ëŸ¼ìœ¼ë¡œ íŒŒí‹°ì…˜ í”„ë£¨ë‹
- ë¶ˆí•„ìš”í•œ í…Œì´ë¸” ì œì™¸
- ì¸ë±ìŠ¤ í‚¤ í™œìš©

**Data Lake ìµœì í™”:**
- ì‹œê³„ì—´ ë°ì´í„°ëŠ” ë‚ ì§œë¡œ íŒŒí‹°ì…”ë‹
- ë©”íƒ€ë°ì´í„° ìºì‹± í™œìš©
- íŒŒì¼ í¬ê¸° ìµœì í™”
- Predicate pushdown í™œìš©

### ğŸš€ í”„ë¡œë•ì…˜ ë°°í¬

#### Best Practices

```sql
-- íŒŒí‹°ì…˜ ì •ë³´ í™•ì¸
SELECT
    partition,
    name,
    rows,
    bytes_on_disk
FROM system.parts
WHERE table = 'your_table'
  AND active = 1;

-- ì‹¤í–‰ ì¤‘ì¸ ì¿¼ë¦¬ ëª¨ë‹ˆí„°ë§
SELECT
    query_id,
    user,
    query,
    elapsed,
    memory_usage
FROM system.processes
WHERE query NOT LIKE '%system.processes%';

-- ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ í™•ì¸
SELECT
    name,
    path,
    formatReadableSize(free_space) AS free,
    formatReadableSize(total_space) AS total
FROM system.disks;
```

### ğŸ›  ê´€ë¦¬ ëª…ë ¹ì–´

#### ClickHouse ê´€ë¦¬

```bash
# ClickHouse ìƒíƒœ í™•ì¸
cd ../oss-mac-setup
./status.sh

# ClickHouse CLI ì ‘ì†
./client.sh 2508

# ClickHouse ì¤‘ì§€
./stop.sh

# ClickHouse ì¬ì‹œì‘
./start.sh
```

#### Data Lake (MinIO + Nessie) ê´€ë¦¬

```bash
# MinIO ì›¹ ì½˜ì†” ì ‘ì†
# ë¸Œë¼ìš°ì €ì—ì„œ: http://localhost:19001
# ìê²©ì¦ëª…: admin / password123

# MinIO ë° Nessie ì¤‘ì§€
cd ../datalake-minio-catalog
docker-compose down

# MinIO ë° Nessie ì¬ì‹œì‘
docker-compose up -d minio nessie minio-setup

# MinIO ë°ì´í„° ì™„ì „ ì‚­ì œ
docker-compose down -v

# ì»¨í…Œì´ë„ˆ ë¡œê·¸ í™•ì¸
docker-compose logs -f minio
docker-compose logs -f nessie
```

#### ì „ì²´ í™˜ê²½ ì¬êµ¬ì„±

```bash
# 1. ëª¨ë“  ì„œë¹„ìŠ¤ ì¤‘ì§€ ë° ì •ë¦¬
cd ../oss-mac-setup
./stop.sh

cd ../datalake-minio-catalog
docker-compose down -v

# 2. ì „ì²´ ì¬ì‹œì‘
cd ../25.8
./00-setup.sh
```

#### ë°ì´í„° í™•ì¸

```bash
# MinIOì— ì €ì¥ëœ íŒŒì¼ í™•ì¸
docker exec -it minio mc ls myminio/warehouse/

# ClickHouse í…Œì´ë¸” í™•ì¸
docker exec -it clickhouse-25-8 clickhouse-client -q "SHOW TABLES"

# ClickHouse ë°ì´í„°ë² ì´ìŠ¤ í¬ê¸° í™•ì¸
docker exec -it clickhouse-25-8 clickhouse-client -q "SELECT database, formatReadableSize(sum(bytes)) as size FROM system.parts GROUP BY database"
```

#### ë§ˆì´ê·¸ë ˆì´ì…˜ ì „ëµ

1. **í…ŒìŠ¤íŠ¸ í™˜ê²½ì—ì„œ ê²€ì¦**
   - ëª¨ë“  ìƒˆ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
   - ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹
   - ë¡¤ë°± ê³„íš ìˆ˜ë¦½

2. **ì ì§„ì  ë¡¤ì•„ì›ƒ**
   - ì‘ì€ ë°ì´í„°ì…‹ë¶€í„° ì‹œì‘
   - ëª¨ë‹ˆí„°ë§ ë° ì„±ëŠ¥ ì¸¡ì •
   - ë¬¸ì œ ë°œìƒ ì‹œ ì¦‰ì‹œ ëŒ€ì‘

3. **ëª¨ë‹ˆí„°ë§**
   - ì¿¼ë¦¬ ì„±ëŠ¥ ì¶”ì 
   - ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
   - ì—ëŸ¬ ë° ê²½ê³  í™•ì¸

### ğŸ¤ ê¸°ì—¬

ì´ ë©ì— ëŒ€í•œ ê°œì„  ì‚¬í•­ì´ë‚˜ ì¶”ê°€ ì˜ˆì œê°€ ìˆë‹¤ë©´:
1. ì´ìŠˆ ë“±ë¡
2. Pull Request ì œì¶œ
3. í”¼ë“œë°± ê³µìœ 

### ğŸ“„ ë¼ì´ì„ ìŠ¤

MIT License - ììœ ë¡­ê²Œ í•™ìŠµ ë° ìˆ˜ì • ê°€ëŠ¥

---

**Happy Learning! ğŸš€**

ì§ˆë¬¸ì´ë‚˜ ì´ìŠˆê°€ ìˆìœ¼ë©´ ë©”ì¸ [clickhouse-hols README](../../README.md)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.
